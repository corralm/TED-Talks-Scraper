{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# TEDscraper Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-24T06:56:51.108234Z",
     "start_time": "2020-02-24T06:56:47.569217Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "## Soup Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-04-24T07:19:30.434915Z",
     "start_time": "2020-04-24T07:19:30.420388Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SoupMaker:\n",
    "    \"\"\"Make soup objects and put your machine to sleep.\"\"\"\n",
    "    \n",
    "\n",
    "    def sleep_short(self):\n",
    "        \"\"\"Suspends execution time between 0 - .2 seconds.\"\"\"\n",
    "        return time.sleep(random.uniform(0, .2))\n",
    "\n",
    "    def sleep_two(self):\n",
    "        \"\"\"Suspends execution time between .5 - 2 seconds.\"\"\"\n",
    "        return time.sleep(random.uniform(.5, 2))\n",
    "    \n",
    "    def sleep_five(self):\n",
    "        \"\"\"Suspends execution time between 3 - 5 seconds.\"\"\"\n",
    "        return time.sleep(random.uniform(3, 5))\n",
    "\n",
    "    def make_soup(self, url):\n",
    "        \"\"\"Returns soup object from a URL.\"\"\"\n",
    "        # generate random user-agent\n",
    "        user_agent = {'User-agent': UserAgent().random}\n",
    "        # request page and make soup\n",
    "        page = requests.get(url, headers=user_agent)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        return soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## CreateCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class CreateCSV(SoupMaker):\n",
    "    \"\"\"Create CSVs of TED topics and languages.\"\"\"\n",
    "\n",
    "\n",
    "    def create_topics_csv(self):\n",
    "        \"\"\"Creates CSV of all topics available from TED.\"\"\"\n",
    "        soup = self.make_soup('https://www.ted.com/topics')\n",
    "        topic_list = []\n",
    "        topic_tag = soup.find_all(class_='d:b', style='line-height:3;')\n",
    "        for tag in topic_tag:\n",
    "            topic = re.sub(r'\\s+', '', tag.text)\n",
    "            topic_list.append(topic)\n",
    "        topics_series = pd.Series(topic_list, name='Topic')\n",
    "        topics_series.to_csv('../data/topics.csv', index=False)\n",
    "\n",
    "    def create_languages_csv(self):\n",
    "        \"\"\"Creates CSV of all language codes supported by TED.\"\"\"\n",
    "        lang_url = 'https://www.ted.com/participate/translate/our-languages'\n",
    "        soup = self.make_soup(lang_url)\n",
    "        lang_list = []\n",
    "        lang_tags = soup.find_all('div', class_='h9')\n",
    "        for tag in lang_tags:\n",
    "            if tag.a == None:\n",
    "                continue\n",
    "            else:\n",
    "                lang_code = re.search(r'(?<=\\=)[\\w-]+', tag.a['href']).group(0)\n",
    "                lang_name = tag.text\n",
    "                lang_list.append([lang_code] + [lang_name])\n",
    "        lang_df = pd.DataFrame(data=lang_list, columns=['lang_code', 'language'])\n",
    "        lang_df.to_csv('../data/languages.csv', index=False)\n",
    "        \n",
    "    def create_dataset(self, DataFrame):\n",
    "        \"\"\"Creates CSV from a DataFrame.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "## Talk Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-04-24T07:19:35.650462Z",
     "start_time": "2020-04-24T07:19:35.604835Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TalkFeatures(SoupMaker):\n",
    "    \"\"\"Class to get TED talk features.\"\"\"\n",
    "\n",
    "\n",
    "    def get_talk_id(self, soup):\n",
    "        \"\"\"Returns the talk_id provided by TED.\"\"\"\n",
    "        talk_id = re.search(r\"(?<=\\\"current_talk\\\":)\\\"(\\d+)\\\"\", soup.text).group(1)\n",
    "        return talk_id\n",
    "\n",
    "    def get_title(self, soup):\n",
    "        \"\"\"Returns the title of the talk.\"\"\"\n",
    "        title_tag = soup.find(attrs={'name': 'title'}).attrs['content']\n",
    "        tag_list = title_tag.split(':')\n",
    "        title = \":\".join(tag_list[1:]).lstrip()\n",
    "        return title\n",
    "\n",
    "    def get_speaker_1(self, soup):\n",
    "        \"\"\"Returns the first speaker in TED's speaker list.\"\"\"\n",
    "        try:\n",
    "            speaker_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?\\\"}]\", soup.text)[0]\n",
    "            # convert to DataFrame\n",
    "            speakers_df = pd.read_json(speaker_tag)\n",
    "            full_name_raw = (speakers_df.loc[:, 'firstname'] + ' '\n",
    "                         + speakers_df.loc[:, 'middleinitial'] + ' '\n",
    "                         + speakers_df.loc[:, 'lastname'])\n",
    "            full_name_clean = full_name_raw.str.replace('\\s+', ' ')\n",
    "            # transform series to a dict\n",
    "            speaker = full_name_clean.iloc[0]\n",
    "        except:\n",
    "            speaker = re.search(r\"(?<=\\\"speaker_name\\\":)\\\"(.*?)\\\"\", soup.text).group(1)\n",
    "        return speaker\n",
    "\n",
    "    def get_all_speakers(self, soup):\n",
    "        \"\"\"Returns dict of all speakers per talk.\"\"\"\n",
    "        try:\n",
    "            speaker_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?\\\"}]\", soup.text)[0]\n",
    "            # convert to DataFrame\n",
    "            speakers_df = pd.read_json(speaker_tag)\n",
    "            full_name_raw = (speakers_df.loc[:, 'firstname'] + ' '\n",
    "                         + speakers_df.loc[:, 'middleinitial'] + ' '\n",
    "                         + speakers_df.loc[:, 'lastname'])\n",
    "            full_name_clean = full_name_raw.str.replace('\\s+', ' ')\n",
    "            # transform series to a dict\n",
    "            speakers = full_name_clean.to_dict()\n",
    "        except:\n",
    "            speakers = None\n",
    "        return speakers\n",
    "\n",
    "    def get_occupations(self, soup):\n",
    "        \"\"\"Returns list of the occupation(s) of the speaker(s) per talk.\"\"\"\n",
    "        try:\n",
    "            occupations_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?\\\"}]\", soup.text)[0]\n",
    "            # convert json to DataFrame\n",
    "            occupations_series = pd.read_json(occupations_tag)['description']\n",
    "            if occupations_series.all():\n",
    "                # clean and create dict\n",
    "                occupations = occupations_series.str.lower().str.split(', ')\n",
    "                occupations = occupations.to_dict()\n",
    "            else:\n",
    "                occupations = None\n",
    "        except:\n",
    "            occupations = None\n",
    "        return occupations\n",
    "\n",
    "    def get_about_speakers(self, soup):\n",
    "        \"\"\"Returns dict with each 'About the Speaker' blurb per talk.\"\"\"\n",
    "        try:\n",
    "            speaker_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?\\\"}]\", soup.text)[0]\n",
    "            # convert to DataFrame\n",
    "            about_series = pd.read_json(speaker_tag)['whotheyare']\n",
    "            if about_series.all():\n",
    "                # transform series to a dict\n",
    "                about_speakers = about_series.to_dict()\n",
    "            else:\n",
    "                about_speakers = None\n",
    "        except:\n",
    "            about_speakers = None\n",
    "        return about_speakers\n",
    "\n",
    "    def get_views(self, soup):\n",
    "        \"\"\"Returns viewed count per talk.\"\"\"\n",
    "        view_count = re.search(r\"(?<=\\\"viewed_count\\\":)\\d+\", soup.text).group(0)\n",
    "        return view_count\n",
    "\n",
    "    def get_recorded_date(self, soup):\n",
    "        \"\"\"Returns date a talk was recorded.\"\"\"\n",
    "        try:\n",
    "            recorded_at = re.search(r\"(?<=\\\"recorded_at\\\":\\\")[\\d-]+\", soup.text).group(0)\n",
    "        except:\n",
    "            recorded_at = None\n",
    "        return recorded_at\n",
    "\n",
    "    def get_published_date(self, soup):\n",
    "        \"\"\"Returns date a talk was published in TED.com.\"\"\"\n",
    "        published_raw = soup.find(attrs={'itemprop': 'uploadDate'}).attrs['content']\n",
    "        published_date = re.search(r\"[\\d-]+\", published_raw).group(0)\n",
    "        return published_date\n",
    "\n",
    "    def get_event(self, soup):\n",
    "        \"\"\"Returns name of the event in which the talk was given.\"\"\"\n",
    "        event = re.search(r\"(?<=\\\"event\\\":)\\\"(.*?)\\\"\", soup.text).group(1)\n",
    "        return event\n",
    "    \n",
    "    def get_native_lang(self, soup):\n",
    "        \"\"\"Returns native language code for each talk as a string.\"\"\"\n",
    "        native_lang = re.search(r'(?<=nativeLanguage\\\":)\\\"(.*?)\\\"', soup.text).group(1)\n",
    "        return native_lang\n",
    "    \n",
    "    def get_available_lang(self, soup):\n",
    "        \"\"\"Returns list of all available languages (lang codes) for a talk.\"\"\"\n",
    "        languages = re.findall(r'(?<=languageCode\\\":)\\\"(.*?)\\\"', soup.text)\n",
    "        clean_lang = sorted(list(set(languages)))\n",
    "        return clean_lang\n",
    "\n",
    "    def get_comments_count(self, soup):\n",
    "        \"\"\"Return the count of comments per talk.\"\"\"\n",
    "        try:\n",
    "            comments_count = re.search(r\"(?<=\\\"count\\\":)(\\d+)\", soup.text).group(1)\n",
    "        except AttributeError:\n",
    "            comments_count = None\n",
    "        return comments_count\n",
    "\n",
    "    def get_duration(self, soup):\n",
    "        \"\"\"Returns duration of a talk in seconds.\"\"\"\n",
    "        duration =  re.search(r\"(?<=\\\"duration\\\":)(\\d+)\", soup.text).group(1)\n",
    "        return duration\n",
    "\n",
    "    def get_topics(self, soup):\n",
    "        \"\"\"Returns list of tags (topics) per talk.\"\"\"\n",
    "        match_obj = re.search(r\"\\\"tag\\\":\\\"(.*?)\\\"\", soup.text)\n",
    "        topics = match_obj.group(1).split(',')\n",
    "        return topics\n",
    "\n",
    "    def get_related_talks(self, soup):\n",
    "        \"\"\"Returns dict (keys: id & title) of related talks.\"\"\"\n",
    "        related_tag = re.search(r\"(?<=\\\"related_talks\\\":).*?]\", soup.text).group(0)\n",
    "        related_sr = pd.read_json(related_tag)\n",
    "        related_talks = dict(zip(related_sr['id'], related_sr['title']))\n",
    "        return related_talks\n",
    "\n",
    "    def get_talk_url(self, soup):\n",
    "        \"\"\"Returns url for each talk as a string.\"\"\"\n",
    "        talk_tag = soup.find(attrs={'property': 'og:url'}).attrs['content']\n",
    "        talk_url = talk_tag.split('transcript')[0]\n",
    "        return talk_url\n",
    "\n",
    "    def get_description(self, soup):\n",
    "        \"\"\"Returns description of the talk.\"\"\"\n",
    "        desc_tag = soup.find(attrs={'property': 'og:description'}).attrs['content']\n",
    "        talk_desc = desc_tag.split(': ', 1)[1]\n",
    "        return talk_desc\n",
    "\n",
    "    def get_transcript(self, soup):\n",
    "        \"\"\"Returns talk's transcript as a single string.\"\"\" \n",
    "        transcript = ''\n",
    "        transcript_strings = []\n",
    "        for div in soup.find_all('div', class_=\"Grid__cell flx-s:1 p-r:4\"):\n",
    "            for p in div.find_all('p'):\n",
    "                # add every string in the transcript to a list\n",
    "                transcript_strings.append(\" \".join(p.text.split()))\n",
    "            else:\n",
    "                # after all strings have been added, create a single transcript string\n",
    "                transcript = \" \".join(transcript_strings)\n",
    "        return transcript\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "## URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-04-24T07:19:36.214636Z",
     "start_time": "2020-04-24T07:19:36.181907Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class URLs(SoupMaker):\n",
    "    \"\"\"Get and process urls to scrape.\"\"\"\n",
    "\n",
    "    \n",
    "    def topics_url_param(self):\n",
    "        \"\"\"Returns string of the url query from topics parameter.\"\"\"\n",
    "        topics_param = ''\n",
    "        if self.topics != 'all':\n",
    "            if isinstance(self.topics, list):\n",
    "                for topic in self.topics:\n",
    "                    topics_param += ('&topics[]=' + topic)\n",
    "            else:\n",
    "                raise ValueError(\"'topics' param needs to be a list\")\n",
    "        return topics_param\n",
    "\n",
    "    def get_max_page(self):\n",
    "        \"\"\"Returns max pagination number from www.ted.com/talks.\"\"\"\n",
    "        page_num = [1]\n",
    "        # make soup from ted.com/talks with specified language\n",
    "        soup = self.make_soup(self.base_url + '&page=1&sort=newest')\n",
    "        # iterate through each pagination element and get the max\n",
    "        page_elem = soup.find_all('a', class_='pagination__item pagination__link')\n",
    "        for element in page_elem:\n",
    "            page_num.append(int(element.text))\n",
    "        return max(page_num)\n",
    "    \n",
    "    def get_all_url_paths(self):\n",
    "        \"\"\"Returns list of all the talk url paths available in www.ted.com/talks\"\"\"\n",
    "        url_path_list = []\n",
    "        # construct url with lang code specified by the user\n",
    "        talks_url = (self.base_url + '&page=')\n",
    "        # set range from 1 to the max page in the pagination element\n",
    "        page_range = range(1, self.get_max_page()+1)\n",
    "        # iterate through each page and get the url for each talk\n",
    "        for i in page_range:\n",
    "            # try a second attempt if first attempt fails\n",
    "            for attempt in range(2):\n",
    "                try:\n",
    "                    talks_page_url = talks_url + str(i) + '&sort=newest'\n",
    "                    soup = self.make_soup(talks_page_url)\n",
    "                    # delay between searches\n",
    "                    self.sleep_short()\n",
    "                    for div in soup.find_all('div', attrs={'class': 'media__image'}):\n",
    "                        for a in div.find_all('a'):\n",
    "                            url_path_list.append(a.get('href'))\n",
    "                except:\n",
    "                    # delay before continuing to second attempt\n",
    "                    self.sleep_two()\n",
    "                # break from attempts loop if no exceptions are raised\n",
    "                else:\n",
    "                    break\n",
    "        return url_path_list\n",
    "\n",
    "    def get_all_urls(self):\n",
    "        \"\"\"Returns list of complete urls for each talk's transcript page.\"\"\"\n",
    "        # '/talks/jen_gunter_why_can_t_we_talk_about_periods?language=fa'\n",
    "        url_list = []\n",
    "        for url in self.get_all_url_paths():\n",
    "            url_list.append(('https://www.ted.com'\n",
    "                             + url.replace(\n",
    "                                 # to replace\n",
    "                                 '?language=' + self.lang_code,\n",
    "                                 # replace with\n",
    "                                 '/transcript' + '?language=' + self.lang_code)\n",
    "                            ))\n",
    "        return url_list\n",
    "    \n",
    "    def clean_urls(self, urls):\n",
    "        \"\"\"Returns list of clean urls from urls the user inputs.\"\"\"\n",
    "        clean_urls = []\n",
    "        for idx, url in enumerate(urls):\n",
    "            if url.startswith('https://www.ted.com/talks'):\n",
    "                parts = url.split('/')\n",
    "                joined = '/'.join(parts[:5])\n",
    "                clean = joined.split('?')\n",
    "                lang = clean[0] + '/transcript?language=' + self.lang_code\n",
    "                topic = lang + self.topics_url_param()\n",
    "                clean_urls.append(lang)\n",
    "            else:\n",
    "                print(f'bad url @ {idx} >> {url}')\n",
    "                continue\n",
    "        return clean_urls\n",
    "    \n",
    "    def url_issues(self):\n",
    "        \"\"\"Returns DataFrame of urls with known issues.\"\"\"\n",
    "        issues_df = pd.read_csv('../data/known_issues.csv')\n",
    "        return issues_df\n",
    "    \n",
    "    def remove_urls_with_issues(self):\n",
    "        \"\"\"Remove urls with known issues to prevent unnecessary scraping.\"\"\"\n",
    "        urls = self.url_attribute()\n",
    "        final_urls = []\n",
    "        removed_urls = []\n",
    "        removed_counter = 0\n",
    "        issues_df = pd.read_csv('../data/known_issues.csv')\n",
    "        for url in urls:\n",
    "            try:\n",
    "                base_url = url.replace('transcript?language=' + self.lang_code, '')\n",
    "                # is base url in the issues df?\n",
    "                url_in_issues = (issues_df['url'] == base_url).any()\n",
    "                # get the lang_codes of the base_url\n",
    "                langs = issues_df.loc[issues_df['url'] == base_url, 'lang_code']\n",
    "                # check if the url in issues_df\n",
    "                if not url_in_issues:\n",
    "                    final_urls.append(url)\n",
    "                # if the url is in issues_df, check if it's for the same lang_code\n",
    "                elif self.lang_code in langs.any():\n",
    "                    removed_urls.append(url)\n",
    "                    removed_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    final_urls.append(url)\n",
    "            except:\n",
    "                removed_urls.append(url)\n",
    "                removed_counter += 1\n",
    "                continue\n",
    "        if removed_urls:\n",
    "            print(f\"Removed the following {removed_counter} urls as they have \"\n",
    "                  \"known issues:\\n\", removed_urls, end='\\n\\n')\n",
    "        return final_urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## TEDscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-04-24T07:32:19.545188Z",
     "start_time": "2020-04-24T07:32:19.494574Z"
    }
   },
   "outputs": [],
   "source": [
    "class TEDscraper(TalkFeatures, URLs):\n",
    "    \"\"\"Gets urls and scrapes TED talk data in the specified language.\n",
    "\n",
    "    Attributes:\n",
    "        lang_code (str): Language code. Defaults to 'en'.\n",
    "        language (str): Language name derived from lang_code.\n",
    "        urls (list): URLs of talks. Defaults to 'all'.\n",
    "        topics (list): Talk topics. Defaults to 'all'.\n",
    "        exclude (bool): Exclude transcript. Defaults to False.\n",
    "        ted_dict (dict): Dict to store ted talk features after scraping.\n",
    "        dict_id (int): Index of nested dict in 'ted_dict'.\n",
    "        failed_counter: Counts urls that failed to get scraped.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, lang_code='en', urls='all', topics='all',\n",
    "                 force_fetch = False, exclude_transcript=False):\n",
    "        self.lang_code = lang_code\n",
    "        self.language = self.convert_lang_code()\n",
    "        self.urls = urls\n",
    "        self.topics = topics\n",
    "        self.exclude = exclude_transcript\n",
    "        self.ted_dict = {}\n",
    "        self.dict_id = 0\n",
    "        self.failed_counter = 0\n",
    "        self.failed_urls = []\n",
    "        self.force_fetch = force_fetch\n",
    "        self.base_url = ('https://www.ted.com/talks'\n",
    "                         + '?language=' + self.lang_code\n",
    "                         + self.topics_url_param())\n",
    "        \n",
    "    def url_attribute(self):\n",
    "        \"\"\"Define urls attribute based on parameter 'urls'.\"\"\"\n",
    "        # define url attribute\n",
    "        if self.urls == 'all':\n",
    "            urls = self.get_all_urls()\n",
    "        else:\n",
    "            if isinstance(self.urls, list):\n",
    "                urls = self.clean_urls(self.urls)\n",
    "            else:\n",
    "                raise ValueError(\"'urls' param needs to be a list\")\n",
    "        return urls\n",
    "\n",
    "    def convert_lang_code(self):\n",
    "        \"\"\"Reads languages.csv and returns language.\n",
    "        Parameters:\n",
    "            lang_code (str): Language code\n",
    "        \"\"\"\n",
    "        df = pd.read_csv('../data/languages.csv')\n",
    "        lang_series = df.loc[(df['lang_code'] == self.lang_code), 'language']\n",
    "        language = lang_series.values[0]\n",
    "        return language\n",
    "    \n",
    "    def scrape_all_features(self, soup):\n",
    "        \"\"\"Scrapes all features to a nested dict.\"\"\"\n",
    "        # create nested dict\n",
    "        self.ted_dict[self.dict_id] = {}\n",
    "        nested_dict = self.ted_dict[self.dict_id]\n",
    "        # add the features to the nested dict\n",
    "        nested_dict['talk_id'] = self.get_talk_id(soup)\n",
    "        nested_dict['title'] = self.get_title(soup)\n",
    "        nested_dict['speaker_1'] = self.get_speaker_1(soup)\n",
    "        nested_dict['all_speakers'] = self.get_all_speakers(soup)\n",
    "        nested_dict['occupations'] = self.get_occupations(soup)\n",
    "        nested_dict['about_speakers'] = self.get_about_speakers(soup)\n",
    "        nested_dict['views'] = self.get_views(soup)\n",
    "        nested_dict['recorded_date'] = self.get_recorded_date(soup)\n",
    "        nested_dict['published_date'] = self.get_published_date(soup)\n",
    "        nested_dict['event'] = self.get_event(soup)\n",
    "        nested_dict['native_lang'] = self.get_native_lang(soup)\n",
    "        nested_dict['available_lang'] = self.get_available_lang(soup)\n",
    "        nested_dict['comments'] = self.get_comments_count(soup)\n",
    "        nested_dict['duration'] = self.get_duration(soup)\n",
    "        nested_dict['topics'] = self.get_topics(soup)\n",
    "        nested_dict['related_talks'] = self.get_related_talks(soup)\n",
    "        nested_dict['url'] = self.get_talk_url(soup)\n",
    "        nested_dict['description'] = self.get_description(soup)\n",
    "        # add transcript if param is set to False (default)\n",
    "        if not self.exclude:\n",
    "            nested_dict['transcript'] = self.get_transcript(soup)\n",
    "        return nested_dict\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"Returns nested dictionary of features from each talk's transcript page.\"\"\"\n",
    "        print(\"Fetching urls...\\n\")\n",
    "        # define url attribute\n",
    "        if self.force_fetch:\n",
    "            urls = self.url_attribute()\n",
    "        else:\n",
    "            urls = self.remove_urls_with_issues()\n",
    "        print(f\"Scraping {len(urls)} TED talks in '{self.language}'...\")\n",
    "        print(f\"Estimated time to complete is {round((.9*len(urls)/60), 1)} minutes\\n\")\n",
    "        # iterate through each ted talk transcript url\n",
    "        for url in urls:\n",
    "            # make soup\n",
    "            soup = self.make_soup(url)\n",
    "            # taste soup\n",
    "            taster = soup.title.text\n",
    "            bad_soup = re.search(r'404: Not Found', taster)\n",
    "            if bad_soup:\n",
    "                print(f\"[BAD_SOUP] {self.lang_code} {url}\")\n",
    "                self.failed_urls.append(url)\n",
    "                self.failed_counter += 1\n",
    "                continue\n",
    "            # delay between searches\n",
    "            self.sleep_short()\n",
    "            # try up to three attempts to scrape data\n",
    "            for attempt in range(1, 3+1):\n",
    "                try:\n",
    "                    # create nested dict\n",
    "                    self.ted_dict[self.dict_id] = {}\n",
    "                    # scrape features and add to a nested dict\n",
    "                    self.scrape_all_features(soup)\n",
    "                    # indicate successful scrape\n",
    "                    print(f\"[OK] {self.dict_id} {url}\")\n",
    "                    # add 1 to create a new nested dict\n",
    "                    self.dict_id += 1\n",
    "                except Exception as e:\n",
    "                    # if the last attempt fails, update the failed counter\n",
    "                    # and print the exception & talk url\n",
    "                    if attempt == 3:\n",
    "                        self.failed_counter += 1\n",
    "                        self.failed_urls.append(url)\n",
    "                        print(f\"[EXCEPTION] {e} {url}\")\n",
    "                        continue\n",
    "                    # delay before another attempt\n",
    "                    self.sleep_five()\n",
    "                # break if no exceptions are raised\n",
    "                else:\n",
    "                    break\n",
    "        print(f\"\"\"\\nTed.com scraping results:\n",
    "            \\n\\t• Successful: {self.dict_id}\n",
    "            \\n\\t• Failed: {self.failed_counter}\\n\"\"\")\n",
    "        if self.failed_counter:\n",
    "            print(f\"Failed to scrape:\\n{self.failed_urls}\\n\")\n",
    "        return self.ted_dict\n",
    "    \n",
    "    def to_dataframe(self, ted_dict):\n",
    "        \"\"\"Returns sorted DataFrame object from dict.\"\"\"\n",
    "        df = pd.DataFrame.from_dict(ted_dict, orient='index')\n",
    "        df = df.sort_values(by='published_date')\n",
    "        sorted_df = df.reset_index(drop=True)\n",
    "        return sorted_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# english\n",
    "# scraper_en = TEDscraper()\n",
    "# ted_dict = scraper_en.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
