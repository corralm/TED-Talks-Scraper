{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Scrape TED Talks Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-24T06:56:51.108234Z",
     "start_time": "2020-02-24T06:56:47.569217Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import random\n",
    "\n",
    "# misc\n",
    "import pickle  # probably won't need this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Notes\n",
    "* **All features** are scraped by default.\n",
    "    * You can exclude scraping the transcript by setting `exclude_transcript` to 'True'.\n",
    "* **English** is the default language.\n",
    "    * You can specify a language using the `lang` param.\n",
    "    * See full list of language codes [here].\n",
    "* **All talks** are scraped by default\n",
    "    * \"All talks\" refers to talks that are listed [here](https://www.ted.com/participate/translate/our-languages 'TED languages') for the language you specify via the `lang` param.\n",
    "    * You may pass in a list of urls to scrape using the `urls` param. However, there are a few limitations:\n",
    "        * TED must have the talks available in the language you specify.\n",
    "        * Only one language can be provided per scrape call.\n",
    "\n",
    "More notes:\n",
    "* TED doesn't always translate all features even if they have it available under a certain language.\n",
    "    * Ex: Title and 'About Speaker' might be in English while the transcript is translated to French\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Features\n",
    "\n",
    "| Feature          | Description                                   | Data Type  |\n",
    "|------------------|-----------------------------------------------|------------|\n",
    "| talk_id          | Talk identification number provided by TED    | int        |\n",
    "| title            | Title of the talk                             | string     |\n",
    "| speakers         | Speakers in the talk (may be multiple)        | dictionary |\n",
    "| occupations      | *Occupations of the speakers (may be multiple) | dictionary |\n",
    "| about_speakers   | *Blurb about each speaker (may be multiple)    | dictionary |\n",
    "| views            | Count of views                                | int        |\n",
    "| recorded_date    | Date the talk was recorded                    | string     |\n",
    "| published_date   | Date the talk was published to TED.com        | string     |\n",
    "| event            | Event or medium in which the talk was given   | string     |\n",
    "| native_lang      | Language the talk was given in                | string     |\n",
    "| available_lang   | All available languages for a talk            | list       |\n",
    "| comments         | Count of comments                             | int        |\n",
    "| duration         | Duration in %M%S format                       | string     |\n",
    "| duration_sec     | Duration in seconds                           | int        |\n",
    "| topic_tags       | Related tags or topics for the talk           | list       |\n",
    "| talk_description | Description of the talk                       | string     |\n",
    "| related_talks    | Related talks                                 | dictionary |\n",
    "| talk_url         | Url of the talk                               | string     |\n",
    "| transcript       | Full transcript of the talk                   | string     |\n",
    "\n",
    "*The dictionary key maps to the speaker in ‘speakers’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Languages\n",
    "TED talks have been subtitled in over 100 languages. You can see the most updated list of talks [here](https://www.ted.com/participate/translate/our-languages 'TED languages').\n",
    "\n",
    "Below is a list of languages available:  \n",
    "\n",
    "| code       | language              |\n",
    "|------------|-----------------------|\n",
    "| af         | Afrikaans             |\n",
    "| sq         | Albanian              |\n",
    "| arq        | Algerian Arabic       |\n",
    "| am         | Amharic               |\n",
    "| ar         | Arabic                |\n",
    "| hy         | Armenian              |\n",
    "| as         | Assamese              |\n",
    "| ast        | Asturian              |\n",
    "| az         | Azerbaijani           |\n",
    "| eu         | Basque                |\n",
    "| be         | Belarusian            |\n",
    "| bn         | Bengali               |\n",
    "| bi         | Bislama               |\n",
    "| bs         | Bosnian               |\n",
    "| bg         | Bulgarian             |\n",
    "| my         | Burmese               |\n",
    "| ca         | Catalan               |\n",
    "| ceb        | Cebuano               |\n",
    "| zh-cn      | Chinese, Simplified   |\n",
    "| zh-tw      | Chinese, Traditional  |\n",
    "| zh         | Chinese, Yue          |\n",
    "| ht         | Creole, Haitian       |\n",
    "| hr         | Croatian              |\n",
    "| cs         | Czech                 |\n",
    "| da         | Danish                |\n",
    "| nl         | Dutch                 |\n",
    "| dz         | Dzongkha              |\n",
    "| en         | English               |\n",
    "| eo         | Esperanto             |\n",
    "| et         | Estonian              |\n",
    "| fil        | Filipino              |\n",
    "| fi         | Finnish               |\n",
    "| fr         | French                |\n",
    "| fr-ca      | French (Canada)       |\n",
    "| gl         | Galician              |\n",
    "| ka         | Georgian              |\n",
    "| de         | German                |\n",
    "| el         | Greek                 |\n",
    "| gu         | Gujarati              |\n",
    "| cnh        | Hakha Chin            |\n",
    "| ha         | Hausa                 |\n",
    "| he         | Hebrew                |\n",
    "| hi         | Hindi                 |\n",
    "| hu         | Hungarian             |\n",
    "| hup        | Hupa                  |\n",
    "| is         | Icelandic             |\n",
    "| ig         | Igbo                  |\n",
    "| id         | Indonesian            |\n",
    "| inh        | Ingush                |\n",
    "| ga         | Irish                 |\n",
    "| it         | Italian               |\n",
    "| ja         | Japanese              |\n",
    "| kn         | Kannada               |\n",
    "| kk         | Kazakh                |\n",
    "| km         | Khmer                 |\n",
    "| tlh        | Klingon               |\n",
    "| ko         | Korean                |\n",
    "| ku         | Kurdish               |\n",
    "| ky         | Kyrgyz                |\n",
    "| lo         | Lao                   |\n",
    "| ltg        | Latgalian             |\n",
    "| la         | Latin                 |\n",
    "| lv         | Latvian               |\n",
    "| lt         | Lithuanian            |\n",
    "| lb         | Luxembourgish         |\n",
    "| rup        | Macedo                |\n",
    "| mk         | Macedonian            |\n",
    "| mg         | Malagasy              |\n",
    "| ms         | Malay                 |\n",
    "| ml         | Malayalam             |\n",
    "| mt         | Maltese               |\n",
    "| mr         | Marathi               |\n",
    "| mfe        | Mauritian Creole      |\n",
    "| mn         | Mongolian             |\n",
    "| srp        | Montenegrin           |\n",
    "| ne         | Nepali                |\n",
    "| nb         | Norwegian Bokmal      |\n",
    "| nn         | Norwegian Nynorsk     |\n",
    "| oc         | Occitan               |\n",
    "| ps         | Pashto                |\n",
    "| fa         | Persian               |\n",
    "| pl         | Polish                |\n",
    "| pt         | Portuguese            |\n",
    "| pt-br      | Portuguese, Brazilian |\n",
    "| pa         | Punjabi               |\n",
    "| ro         | Romanian              |\n",
    "| ru         | Russian               |\n",
    "| ry         | Rusyn                 |\n",
    "| sc         | Sardinian             |\n",
    "| sr         | Serbian               |\n",
    "| sh         | Serbo-Croatian        |\n",
    "| szl        | Silesian              |\n",
    "| si         | Sinhala               |\n",
    "| sk         | Slovak                |\n",
    "| sl         | Slovenian             |\n",
    "| so         | Somali                |\n",
    "| es         | Spanish               |\n",
    "| sw         | Swahili               |\n",
    "| sv         | Swedish               |\n",
    "| art-x-bork | Swedish Chef          |\n",
    "| tl         | Tagalog               |\n",
    "| tg         | Tajik                 |\n",
    "| ta         | Tamil                 |\n",
    "| tt         | Tatar                 |\n",
    "| te         | Telugu                |\n",
    "| th         | Thai                  |\n",
    "| bo         | Tibetan               |\n",
    "| aeb        | Tunisian Arabic       |\n",
    "| tr         | Turkish               |\n",
    "| tk         | Turkmen               |\n",
    "| uk         | Ukrainian             |\n",
    "| ur         | Urdu                  |\n",
    "| ug         | Uyghur                |\n",
    "| uz         | Uzbek                 |\n",
    "| vi         | Vietnamese            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\"\"\"TedScraper\n",
    "Created: 2020-04-15\n",
    "Updated: –\n",
    "Author: Miguel Corral Jr.\n",
    "\"\"\"\n",
    "\n",
    "class SoupMaker:\n",
    "    \"\"\"HOLDER 'www.ted.com/talks'.\"\"\"\n",
    "\n",
    "    def sleep_short(self):\n",
    "        \"\"\"Suspends execution time between .5 - 2 seconds.\"\"\"\n",
    "        return time.sleep(random.uniform(.5, 2))\n",
    "\n",
    "    def sleep_long(self):\n",
    "        \"\"\"Suspends execution time between 4 - 6 seconds.\"\"\"\n",
    "        return time.sleep(random.uniform(4, 6))\n",
    "    \n",
    "    def make_soup(self, url):\n",
    "        \"\"\"Returns soup object from a URL.\"\"\"\n",
    "        # generate random user-agent\n",
    "        user_agent = {'User-agent': UserAgent().random}\n",
    "        # request page and make soup\n",
    "        page = requests.get(url, headers=user_agent)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class TalkFeatures(SoupMaker):\n",
    "    def get_talk_id(self, soup):\n",
    "        \"\"\"Returns the talk_id provided by TED.\"\"\"\n",
    "        talk_id = re.search(r\"(?<=\\\"current_talk\\\":)\\\"(\\d+)\\\"\", soup.text).group(1)\n",
    "        return talk_id\n",
    "\n",
    "    def get_title(self, soup):\n",
    "        \"\"\"Returns the title of the talk.\"\"\"\n",
    "        title_tag = soup.find(attrs={'name': 'title'}).attrs['content']\n",
    "        title = title_tag.split(':')[1].strip()\n",
    "        return title\n",
    "\n",
    "    def get_speakers(self, soup):\n",
    "        \"\"\"Returns dict of all speakers per talk.\"\"\"\n",
    "        speaker_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?]\", soup.text)[0]\n",
    "        # convert to DataFrame\n",
    "        speakers_df = pd.read_json(speaker_tag)\n",
    "        full_name_raw = (speakers_df.loc[:, 'firstname'] + ' '\n",
    "                     + speakers_df.loc[:, 'middleinitial'] + ' '\n",
    "                     + speakers_df.loc[:, 'lastname'])\n",
    "        full_name_clean = full_name_raw.str.replace('\\s+', ' ')\n",
    "        # transform series to a dict\n",
    "        speakers = full_name_clean.to_dict()\n",
    "        return speakers\n",
    "\n",
    "    def get_occupations(self, soup):\n",
    "        \"\"\"Returns list of the occupation(s) of the speaker(s) per talk.\"\"\"\n",
    "        occupations_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?]\", soup.text)[0]\n",
    "        # convert json to DataFrame\n",
    "        occupations_series = pd.read_json(occupations_tag)['description']\n",
    "        if occupations_series.all():\n",
    "            # clean and create dict\n",
    "            occupations = occupations_series.str.lower().str.split(', ')\n",
    "            occupations = occupations.to_dict()\n",
    "        else:\n",
    "            occupations = None\n",
    "        return occupations\n",
    "\n",
    "    def get_about_speakers(self, soup):\n",
    "        \"\"\"Returns dict with each 'About the Speaker' blurb per talk.\"\"\"\n",
    "        speaker_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?]\", soup.text)[0]\n",
    "        # convert to DataFrame\n",
    "        about_series = pd.read_json(speaker_tag)['whotheyare']\n",
    "        if about_series.all():\n",
    "            # transform series to a dict\n",
    "            about_speakers = about_series.to_dict()\n",
    "        else:\n",
    "            about_speakers = None\n",
    "        return about_speakers\n",
    "\n",
    "    def get_views(self, soup):\n",
    "        \"\"\"Returns viewed count per talk.\"\"\"\n",
    "        view_count = re.search(r\"(?<=\\\"viewed_count\\\":)\\d+\", soup.text).group(0)\n",
    "        return view_count\n",
    "\n",
    "    def get_recorded_date(self, soup):\n",
    "        \"\"\"Returns date a talk was recorded.\"\"\"\n",
    "        recorded_at = re.search(r\"(?<=\\\"recorded_at\\\":)\\\"(.*?)T\", soup.text).group(1)\n",
    "        return recorded_at\n",
    "\n",
    "    def get_published_date(self, soup):\n",
    "        \"\"\"Returns date a talk was published in TED.com.\"\"\"\n",
    "        published_at = soup.find(attrs={'itemprop': 'uploadDate'}).attrs['content']\n",
    "        return published_at\n",
    "\n",
    "    def get_event(self, soup):\n",
    "        \"\"\"Returns name of the event in which the talk was given.\"\"\"\n",
    "        event = re.search(r\"(?<=\\\"event\\\":)\\\"(.*?)\\\"\", soup.text).group(1)\n",
    "        return event\n",
    "    \n",
    "    def get_native_lang(self, soup):\n",
    "        \"\"\"Returns native language code for each talk as a string.\"\"\"\n",
    "        native_lang = re.search(r'(?<=nativeLanguage\\\":\\\")[\\w-]+', soup.text).group(0)\n",
    "        return native_lang\n",
    "    \n",
    "    def get_available_lang(self, soup):\n",
    "        \"\"\"Returns list of all available languages (lang codes) for a talk.\"\"\"\n",
    "        languages = re.findall(r'(?<=languageCode\\\":\\\")[\\w-]+', soup.text)\n",
    "        clean_lang = sorted(list(set(languages)))\n",
    "        return clean_lang\n",
    "\n",
    "    def get_comments_count(self, soup):\n",
    "        \"\"\"Return the count of comments per talk.\"\"\"\n",
    "        try:\n",
    "            comments_count = re.search(r\"(?<=\\\"count\\\":)(\\d+)\", soup.text).group(1)\n",
    "        except AttributeError:\n",
    "            comments_count = None\n",
    "        return comments_count\n",
    "\n",
    "    def get_duration(self, soup):\n",
    "        \"\"\"Returns duration of a talk (format ex: 12M43S)\"\"\"\n",
    "        duration_tag = soup.find(attrs={'itemprop': 'duration'}).attrs['content']\n",
    "        duration = duration_tag.split('PT')[1]\n",
    "        return duration\n",
    "\n",
    "    def get_duration_sec(self, soup):\n",
    "        \"\"\"Returns duration of a talk in seconds.\"\"\"\n",
    "        duration =  re.search(r\"(?<=\\\"duration\\\":)(\\d+)\", soup.text).group(1)\n",
    "        return duration\n",
    "\n",
    "    def get_topic_tags(self, soup):\n",
    "        \"\"\"Returns list of tags (topics) per talk.\"\"\"\n",
    "        match_obj = re.search(r\"\\\"tag\\\":\\\"(.*?)\\\"\", soup.text)\n",
    "        tags = match_obj.group(1).split(',')\n",
    "        return tags\n",
    "    \n",
    "    def get_related_talks(self, soup):\n",
    "        \"\"\"Returns dict (keys: id & title) of related talks.\"\"\"\n",
    "        related_tag = re.search(r\"(?<=\\\"related_talks\\\":).*?]\", soup.text).group(0)\n",
    "        related_series = pd.read_json(related_tag)\n",
    "        related_talks = related_series.loc[:, ['id', 'title']].to_dict()\n",
    "        return related_talks\n",
    "\n",
    "    def get_talk_url(self, soup):\n",
    "        \"\"\"Returns url for each talk as a string.\"\"\"\n",
    "        talk_tag = soup.find(attrs={'property': 'og:url'}).attrs['content']\n",
    "        talk_url = talk_tag.split('/transcript')[0]\n",
    "        return talk_url\n",
    "\n",
    "    def get_talk_description(self, soup):\n",
    "        \"\"\"Returns description of the talk.\"\"\"\n",
    "        desc_tag = soup.find(attrs={'property': 'og:description'}).attrs['content']\n",
    "        talk_desc = desc_tag.split(': ', 1)[1]\n",
    "        return talk_desc\n",
    "\n",
    "    def get_transcript(self, soup):\n",
    "        \"\"\"Returns talk's transcript as a single string.\"\"\" \n",
    "        transcript = ''\n",
    "        transcript_strings = []\n",
    "        for div in soup.find_all('div', class_=\"Grid__cell flx-s:1 p-r:4\"):\n",
    "            for p in div.find_all('p'):\n",
    "                # add every string in the transcript to a list\n",
    "                transcript_strings.append(\" \".join(p.text.split()))\n",
    "            else:\n",
    "                # after all strings have been added, create a single transcript string\n",
    "                transcript = \" \".join(transcript_strings)\n",
    "        return transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class TEDscraper(TalkFeatures):\n",
    "    def __init__(self, urls='all', lang='en', exclude_transcript=False):\n",
    "        self.lang = lang\n",
    "        self.urls = urls\n",
    "        self.exclude = exclude_transcript\n",
    "        self.ted_dict = {}\n",
    "        self.dict_id = 0\n",
    "        self.failed_counter = 0\n",
    "\n",
    "    def get_languages(self):\n",
    "        \"\"\"Returns DataFrame of all language codes supported by TED.\"\"\"\n",
    "        lang_url = 'https://www.ted.com/participate/translate/our-languages'\n",
    "        soup = self.make_soup(lang_url)\n",
    "        lang_list = []\n",
    "        lang_tags = soup.find_all('div', class_='h9')\n",
    "        for tag in lang_tags:\n",
    "            if tag.a == None:\n",
    "                continue\n",
    "            else:\n",
    "                lang_code = re.search(r'(?<=\\=)[\\w-]+', tag.a['href']).group(0)\n",
    "                lang_name = tag.text\n",
    "                lang_list.append([lang_code] + [lang_name])\n",
    "        lang_df = pd.DataFrame(data=lang_list, columns=['code', 'language'])\n",
    "        return lang_df\n",
    "\n",
    "    def get_max_page(self):\n",
    "        \"\"\"Returns max pagination number from www.ted.com/talks.\"\"\"\n",
    "        page_num = [1]\n",
    "        # make soup from ted.com/talks with specified language\n",
    "        soup = self.make_soup('https://www.ted.com/talks?language='\n",
    "                              + self.lang + '&page=1&sort=newest')\n",
    "        # iterate through each pagination element and get the max\n",
    "        page_elem = soup.find_all('a', class_='pagination__item pagination__link')\n",
    "        for element in page_elem:\n",
    "            page_num.append(int(element.text))\n",
    "        return max(page_num)\n",
    "    \n",
    "    def get_all_url_paths(self):\n",
    "        \"\"\"Returns list of all the talk url paths available in www.ted.com/talks\"\"\"\n",
    "        url_path_list = []\n",
    "        # construct url with lang code specified by the user\n",
    "        talks_url = ('https://www.ted.com/talks?language='\n",
    "                    + self.lang + '&page=')\n",
    "        # set range from 1 to the max page in the pagination element\n",
    "        page_range = range(self.get_max_page())\n",
    "        # iterate through each page and get the url for each talk\n",
    "        for i in page_range:\n",
    "            # try a second attempt if first attempt fails\n",
    "            for attempt in range(2):\n",
    "                try:\n",
    "                    talks_page_url = talks_url + str(i) + '&sort=newest'\n",
    "                    soup = self.make_soup(talks_page_url)\n",
    "                    # delay between searches\n",
    "                    self.sleep_short()\n",
    "                    for div in soup.find_all('div', attrs={'class': 'media__image'}):\n",
    "                        for a in div.find_all('a'):\n",
    "                            url_path_list.append(a.get('href'))\n",
    "                except:\n",
    "                    # delay before continuing to second attempt\n",
    "                    self.sleep_long()\n",
    "                # break from attempts loop if no exceptions are raised\n",
    "                else:\n",
    "                    break\n",
    "        return url_path_list\n",
    "\n",
    "    def get_all_urls(self):\n",
    "        \"\"\"Returns list of complete urls for each talk's transcript page.\"\"\"\n",
    "        url_list = []\n",
    "        for url in self.get_all_url_paths():\n",
    "            url_list.append(('https://www.ted.com'\n",
    "                             + url.replace(\n",
    "                                 # to replace\n",
    "                                 '?language=' + self.lang,\n",
    "                                 # replace with\n",
    "                                 '/transcript' + '?language=' + self.lang)\n",
    "                            ))\n",
    "        return url_list\n",
    "    \n",
    "    def clean_urls(self, urls):\n",
    "        \"\"\"Returns list of clean urls from urls the user inputs.\"\"\"\n",
    "        clean_urls = []\n",
    "        for idx, url in enumerate(urls):\n",
    "            if url.startswith('https://www.ted.com/talks'):\n",
    "                parts = url.split('/')\n",
    "                joined = '/'.join(parts[:5])\n",
    "                clean = joined.split('?')\n",
    "                lang = clean[0] + '/transcript?language=' + self.lang\n",
    "                clean_urls.append(lang)\n",
    "            else:\n",
    "                print(f'bad url @ {idx} >> {url}')\n",
    "                continue\n",
    "        return clean_urls\n",
    "    \n",
    "    def scrape_all_features(self, soup):\n",
    "        \"\"\"Scrapes all features to a nested dict.\"\"\"\n",
    "        # create nested dict\n",
    "        self.ted_dict[self.dict_id] = {}\n",
    "        nested_dict = self.ted_dict[self.dict_id]\n",
    "        # add the features to the nested dict\n",
    "        nested_dict['talk_id'] = self.get_talk_id(soup)\n",
    "        nested_dict['title'] = self.get_title(soup)\n",
    "        nested_dict['speakers'] = self.get_speakers(soup)\n",
    "        nested_dict['occupations'] = self.get_occupations(soup)\n",
    "        nested_dict['about_speakers'] = self.get_about_speakers(soup)\n",
    "        nested_dict['views'] = self.get_views(soup)\n",
    "        nested_dict['recorded_date'] = self.get_recorded_date(soup)\n",
    "        nested_dict['published_date'] = self.get_published_date(soup)\n",
    "        nested_dict['event'] = self.get_event(soup)\n",
    "        nested_dict['native_lang'] = self.get_native_lang(soup)\n",
    "        nested_dict['available_lang'] = self.get_available_lang(soup)\n",
    "        nested_dict['comments'] = self.get_comments_count(soup)\n",
    "        nested_dict['duration'] = self.get_duration(soup)\n",
    "        nested_dict['duration_sec'] = self.get_duration_sec(soup)\n",
    "        nested_dict['topic_tags'] = self.get_topic_tags(soup)\n",
    "        nested_dict['related_talks'] = self.get_related_talks(soup)\n",
    "        nested_dict['talk_url'] = self.get_talk_url(soup)\n",
    "        nested_dict['talk_description'] = self.get_talk_description(soup)\n",
    "        # add transcript if param is set to False (default)\n",
    "        if not self.exclude:\n",
    "            nested_dict['transcript'] = self.get_transcript(soup)\n",
    "        return nested_dict\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"Returns nested dictionary of features from each talk's transcript page.\"\"\"\n",
    "        print(\"Getting all urls...\")\n",
    "        # define url attribute\n",
    "        if self.urls == 'all':\n",
    "            urls = self.get_all_urls()\n",
    "        else:\n",
    "            if isinstance(self.urls, list):\n",
    "                urls = self.clean_urls(self.urls)\n",
    "            else:\n",
    "                print(\"'urls' param needs to be a list\")\n",
    "        print(f\"Scraping {len(urls)} TED talks in '{self.lang}'...\")\n",
    "#         print(f\"Estimated scrape time is {(1.5*len(urls)/60)} minutes\\n\")\n",
    "        # iterate through each ted talk transcript url\n",
    "        for url in urls:\n",
    "            # make soup\n",
    "            soup = self.make_soup(url)\n",
    "            # taste soup\n",
    "            taster = soup.title.text\n",
    "            bad_soup = re.search(r'404: Not Found', taster)\n",
    "            if bad_soup:\n",
    "                print(f\"\\nBad soup! TED might not have this talk available in \"\n",
    "                      f\"'{self.lang}'. Check the url\\n{url}\\n\")\n",
    "                self.failed_counter += 1\n",
    "                continue\n",
    "            # delay between searches\n",
    "            self.sleep_short()\n",
    "            # try up to three attempts to scrape data\n",
    "            for attempt in range(1, 3+1):\n",
    "                try:\n",
    "                    # create nested dict\n",
    "                    self.ted_dict[self.dict_id] = {}\n",
    "                    # scrape features and add to a nested dict\n",
    "                    self.scrape_all_features(soup)\n",
    "                    # indicate successful scrape\n",
    "                    print(self.dict_id, url)\n",
    "                    # add 1 to create a new nested dict\n",
    "                    self.dict_id += 1\n",
    "                except Exception as e:\n",
    "                    # if the last attempt fails, update the failed counter\n",
    "                    # and print the exception & talk url\n",
    "                    if attempt == 3:\n",
    "                        self.failed_counter += 1\n",
    "                        print(f'position: {self.dict_id}, exception: {e}, url: {url}\\n')\n",
    "                        continue\n",
    "                    # delay before another attempt\n",
    "                    self.sleep_long()\n",
    "                # break if no exceptions are raised\n",
    "                else:\n",
    "                    break\n",
    "        print(f\"\"\"\\nTed.com scraping results:\n",
    "            \\n\\t• Successful: {self.dict_id}\n",
    "            \\n\\t• Failed: {self.failed_counter}\\n\"\"\")\n",
    "        return self.ted_dict\n",
    "        return soup\n",
    "    \n",
    "    def to_dataframe(self, ted_dict):\n",
    "        \"\"\"Creates DataFrame object from dict.\"\"\"\n",
    "        df = pd.DataFrame.from_dict(ted_dict, orient='index')\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Output types\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#serialization-io-conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# get languages\n",
    "test = TEDscraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://www.ted.com/talks/sarah_kaminsky_my_father_the_forger/',\n",
    "    'https://www.ted.com/talks/jorge_drexler_poetry_music_and_identity/transcript',\n",
    "    'https://www.ted.com/talks/sir_ken_robinson_do_schools_kill_creativity/',\n",
    "    'https://www.ted.com/talks/paul_mceuen_and_marc_miskin_tiny_robots_with_giant_potential/transcript',\n",
    "    'https://www.ted.com/talks/antara_raychaudhuri_and_iseult_gillespie_the_legend_of_annapurna_hindu_goddess_of_nourishment/',\n",
    "    'https://www.ted.com/talks/diana_reiss_peter_gabriel_neil_gershenfeld_and_vint_cerf_the_interspecies_internet_an_idea_in_progress/',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "url = ['https://www.ted.com/talks/diana_reiss_peter_gabriel_neil_gershenfeld_and_vint_cerf_the_interspecies_internet_an_idea_in_progress/']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "get_speakers_2(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "test = TEDscraper(urls=url)\n",
    "test.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Troubleshooting\n",
    "* check if len speakers is len speaker occupations\n",
    "    * failed: 'https://www.ted.com/talks/adam_kucharski_how_can_we_control_the_coronavirus_pandemic/transcript?language=ru'\n",
    "* what if language is NOT available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    \"\"\"Make soup for each Ted Talk transcript url.\"\"\"\n",
    "    # generate random user-agent\n",
    "    user_agent = {'User-agent': UserAgent().random}\n",
    "    \n",
    "    # request page and make soup\n",
    "    page = requests.get(url, headers=user_agent)\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_max_page():\n",
    "    \"\"\"Return the max pagination number from Ted's home page.\"\"\"\n",
    "    page_num = []\n",
    "    \n",
    "    # set language to English and sort by newest talks\n",
    "    soup = make_soup('https://www.ted.com/talks?language=en&page=1&sort=newest')\n",
    "    \n",
    "    # iterate through each pagination element and get the max\n",
    "    for element in soup.find_all('a', class_='pagination__item pagination__link'):\n",
    "        page_num.append(int(element.text))\n",
    "    return max(page_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_talk_urls():\n",
    "    \"\"\"Return all the talk urls in each talks page.\n",
    "    Filter for talks in English.\n",
    "    \"\"\"\n",
    "    talk_url_list = []\n",
    "    \n",
    "    # set language to English\n",
    "    talks_url = 'https://www.ted.com/talks?language=en&page='\n",
    "    \n",
    "    # set range from 1 to the max page in the pagination element\n",
    "    page_range = range(1, get_max_page()+1)\n",
    "    \n",
    "    # iterate through each page and get the url for each talk\n",
    "    for i in page_range:\n",
    "        \n",
    "        # try a second attempt if first attempt fails\n",
    "        for attempt in range(2):\n",
    "            try:\n",
    "                talks_page_url = talks_url + str(i) + '&sort=newest'\n",
    "                soup = make_soup(talks_page_url)\n",
    "\n",
    "                # delay between searches\n",
    "                time.sleep(random.uniform(1, 2))\n",
    "\n",
    "                for div in soup.find_all('div', attrs={'class': 'media__image'}):\n",
    "                    for a in div.find_all('a'):\n",
    "                        talk_url_list.append(a.get('href'))\n",
    "            except:\n",
    "                # delay before continuing to second attempt\n",
    "                time.sleep(random.uniform(2, 3))\n",
    "            \n",
    "            # break if no exceptions are raised\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    return talk_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def construct_url():\n",
    "    \"\"\"Construct complete url for each talk's transcript page.\n",
    "    Remove '?language=en' from the end of the url.\n",
    "    \"\"\"\n",
    "    return ['https://www.ted.com' + url.replace('?language=en', '') + '/transcript' for url in get_talk_urls()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"Return nested dictionary with title, speaker, related tags, and transcript from each talk's transcript page.\"\"\"\n",
    "    # create empty dict to nest dicts (1 nested dict per talk)\n",
    "    ted_dict = {}\n",
    "    \n",
    "    # 1. used to create nested dicts in 'ted_dict'\n",
    "    # 2. used as count for successfully scraped pages\n",
    "    dict_id = 0\n",
    "    \n",
    "    # counter for failed scraped pages\n",
    "    failed_counter = 0\n",
    "    \n",
    "    # iterate through each ted talk transcript url\n",
    "    for url in construct_url():\n",
    "        # make soup\n",
    "        soup = make_soup(url)\n",
    "\n",
    "        # delay between searches\n",
    "        time.sleep(random.uniform(.5, 2))\n",
    "        \n",
    "        # try up to three attempts to scrape data\n",
    "        for attempt in range(1, 3+1):\n",
    "            try:    \n",
    "                # get title\n",
    "                title_tag = soup.find(attrs={'name': 'title'}).attrs['content']\n",
    "                title = title_tag.split(':')[1].strip()\n",
    "\n",
    "                # get speaker\n",
    "                speaker = soup.find(attrs={'name': 'author'}).attrs['content']\n",
    "\n",
    "                # get related tags\n",
    "                match_obj = re.search(r\"\\\"tag\\\":\\\"(.*?)\\\"\", soup.text)\n",
    "                ted_tags = match_obj.group(1).split(',')\n",
    "\n",
    "                # get talk description\n",
    "                desc_tag = soup.find(attrs={'property': 'og:description'}).attrs['content']\n",
    "                # Split description at \"TED Talk Subtitles and Transcript:\"\n",
    "                desc_str = desc_tag.split(': ', 1)[1]\n",
    "\n",
    "                # get transcript\n",
    "                transcript = ''\n",
    "                transcript_strings = []\n",
    "                for div in soup.find_all('div', class_=\"Grid__cell flx-s:1 p-r:4\"):\n",
    "                    for p in div.find_all('p'):\n",
    "                        # Add every string in the transcript to a list\n",
    "                        transcript_strings.append(\" \".join(p.text.split()))\n",
    "                    else:\n",
    "                        # after all strings have been added, create a single transcript string\n",
    "                        transcript = \" \".join(transcript_strings)\n",
    "\n",
    "                # add 1 to create a new dict_id and use it to create a nested dict\n",
    "                dict_id += 1\n",
    "                ted_dict[dict_id] = {}\n",
    "\n",
    "                # add the features above to the nested dict\n",
    "                ted_dict[dict_id]['title'] = title\n",
    "                ted_dict[dict_id]['speaker'] = speaker\n",
    "                ted_dict[dict_id]['tags'] = ted_tags\n",
    "                ted_dict[dict_id]['description'] = desc_str\n",
    "                ted_dict[dict_id]['transcript'] = transcript\n",
    "                \n",
    "                # indicate successfull scrape\n",
    "                print(dict_id, url)\n",
    "\n",
    "            except Exception as e:\n",
    "                # if the last attempt fails, update the failed counter and print the exception & talk url for debugging\n",
    "                if attempt == 3:\n",
    "                    failed_counter += 1\n",
    "                    print(f'position: {dict_id}, exception: {e}, url: {url}\\n')\n",
    "                    continue\n",
    "                \n",
    "                # delay before another attempt\n",
    "                time.sleep(random.uniform(4, 6))\n",
    "            \n",
    "            # break if no exceptions are raised\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    print(f\"\"\"Ted.com scraping results:\n",
    "        \\n\\t• Success: {dict_id}\n",
    "        \\n\\t• Failed: {failed_counter}\\n\"\"\")\n",
    "    \n",
    "    return ted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# call web scraping function\n",
    "# ted_dict = get_data()\n",
    "# print(ted_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save pickle\n",
    "# with open('data/ted_dict_complete.pkl', 'wb') as f:\n",
    "#     pickle.dump(ted_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-24T01:14:56.451074Z",
     "start_time": "2020-02-24T01:14:56.245272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(ted_dict, orient='index')\n",
    "\n",
    "# Pickle DataFrame\n",
    "df.to_pickle('data/first_df.pkl')\n",
    "\n",
    "print(f'Shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "We were able to successfully scrape 3904 talks (as of February 21, 2020)!  \n",
    "\n",
    "Only one talk failed as the transcript doesn't load, maybe we should notify TED about this bug...  \n",
    "https://www.ted.com/talks/marcus_bullock_an_app_that_helps_incarcerated_people_stay_connected_to_their_families/transcript\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
