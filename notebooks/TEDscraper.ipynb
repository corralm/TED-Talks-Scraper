{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# TEDscraper Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-24T06:56:51.108234Z",
     "start_time": "2020-02-24T06:56:47.569217Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## Soup Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class SoupMaker:\n",
    "    \"\"\"Make soup objects and put your machine to sleep.\"\"\"\n",
    "    def sleep_short(self):\n",
    "        \"\"\"Suspends execution time between .5 - 2 seconds.\"\"\"\n",
    "        return time.sleep(random.uniform(.5, 2))\n",
    "\n",
    "    def sleep_long(self):\n",
    "        \"\"\"Suspends execution time between 4 - 6 seconds.\"\"\"\n",
    "        return time.sleep(random.uniform(4, 6))\n",
    "\n",
    "    def make_soup(self, url):\n",
    "        \"\"\"Returns soup object from a URL.\"\"\"\n",
    "        # generate random user-agent\n",
    "        user_agent = {'User-agent': UserAgent().random}\n",
    "        # request page and make soup\n",
    "        page = requests.get(url, headers=user_agent)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        return soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Talk Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class TalkFeatures(SoupMaker):\n",
    "    \"\"\"Class to get TED talk features.\"\"\"\n",
    "    def get_talk_id(self, soup):\n",
    "        \"\"\"Returns the talk_id provided by TED.\"\"\"\n",
    "        talk_id = re.search(r\"(?<=\\\"current_talk\\\":)\\\"(\\d+)\\\"\", soup.text).group(1)\n",
    "        return talk_id\n",
    "\n",
    "    def get_title(self, soup):\n",
    "        \"\"\"Returns the title of the talk.\"\"\"\n",
    "        title_tag = soup.find(attrs={'name': 'title'}).attrs['content']\n",
    "        title = title_tag.split(':')[1].strip()\n",
    "        return title\n",
    "\n",
    "    def get_speakers(self, soup):\n",
    "        \"\"\"Returns dict of all speakers per talk.\"\"\"\n",
    "        speaker_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?\\\"}]\", soup.text)[0]\n",
    "        # convert to DataFrame\n",
    "        speakers_df = pd.read_json(speaker_tag)\n",
    "        full_name_raw = (speakers_df.loc[:, 'firstname'] + ' '\n",
    "                     + speakers_df.loc[:, 'middleinitial'] + ' '\n",
    "                     + speakers_df.loc[:, 'lastname'])\n",
    "        full_name_clean = full_name_raw.str.replace('\\s+', ' ')\n",
    "        # transform series to a dict\n",
    "        speakers = full_name_clean.to_dict()\n",
    "        return speakers\n",
    "\n",
    "    def get_occupations(self, soup):\n",
    "        \"\"\"Returns list of the occupation(s) of the speaker(s) per talk.\"\"\"\n",
    "        occupations_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?\\\"}]\", soup.text)[0]\n",
    "        # convert json to DataFrame\n",
    "        occupations_series = pd.read_json(occupations_tag)['description']\n",
    "        if occupations_series.all():\n",
    "            # clean and create dict\n",
    "            occupations = occupations_series.str.lower().str.split(', ')\n",
    "            occupations = occupations.to_dict()\n",
    "        else:\n",
    "            occupations = None\n",
    "        return occupations\n",
    "\n",
    "    def get_about_speakers(self, soup):\n",
    "        \"\"\"Returns dict with each 'About the Speaker' blurb per talk.\"\"\"\n",
    "        speaker_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?\\\"}]\", soup.text)[0]\n",
    "        # convert to DataFrame\n",
    "        about_series = pd.read_json(speaker_tag)['whotheyare']\n",
    "        if about_series.all():\n",
    "            # transform series to a dict\n",
    "            about_speakers = about_series.to_dict()\n",
    "        else:\n",
    "            about_speakers = None\n",
    "        return about_speakers\n",
    "\n",
    "    def get_views(self, soup):\n",
    "        \"\"\"Returns viewed count per talk.\"\"\"\n",
    "        view_count = re.search(r\"(?<=\\\"viewed_count\\\":)\\d+\", soup.text).group(0)\n",
    "        return view_count\n",
    "\n",
    "    def get_recorded_date(self, soup):\n",
    "        \"\"\"Returns date a talk was recorded.\"\"\"\n",
    "        recorded_at = re.search(r\"(?<=\\\"recorded_at\\\":)\\\"(.*?)T\", soup.text).group(1)\n",
    "        return recorded_at\n",
    "\n",
    "    def get_published_date(self, soup):\n",
    "        \"\"\"Returns date a talk was published in TED.com.\"\"\"\n",
    "        published_at = soup.find(attrs={'itemprop': 'uploadDate'}).attrs['content']\n",
    "        return published_at\n",
    "\n",
    "    def get_event(self, soup):\n",
    "        \"\"\"Returns name of the event in which the talk was given.\"\"\"\n",
    "        event = re.search(r\"(?<=\\\"event\\\":)\\\"(.*?)\\\"\", soup.text).group(1)\n",
    "        return event\n",
    "    \n",
    "    def get_native_lang(self, soup):\n",
    "        \"\"\"Returns native language code for each talk as a string.\"\"\"\n",
    "        native_lang = re.search(r'(?<=nativeLanguage\\\":\\\")[\\w-]+', soup.text).group(0)\n",
    "        return native_lang\n",
    "    \n",
    "    def get_available_lang(self, soup):\n",
    "        \"\"\"Returns list of all available languages (lang codes) for a talk.\"\"\"\n",
    "        languages = re.findall(r'(?<=languageCode\\\":\\\")[\\w-]+', soup.text)\n",
    "        clean_lang = sorted(list(set(languages)))\n",
    "        return clean_lang\n",
    "\n",
    "    def get_comments_count(self, soup):\n",
    "        \"\"\"Return the count of comments per talk.\"\"\"\n",
    "        try:\n",
    "            comments_count = re.search(r\"(?<=\\\"count\\\":)(\\d+)\", soup.text).group(1)\n",
    "        except AttributeError:\n",
    "            comments_count = None\n",
    "        return comments_count\n",
    "\n",
    "    def get_duration(self, soup):\n",
    "        \"\"\"Returns duration of a talk (format ex: 12M43S)\"\"\"\n",
    "        duration_tag = soup.find(attrs={'itemprop': 'duration'}).attrs['content']\n",
    "        duration = duration_tag.split('PT')[1]\n",
    "        return duration\n",
    "\n",
    "    def get_duration_sec(self, soup):\n",
    "        \"\"\"Returns duration of a talk in seconds.\"\"\"\n",
    "        duration =  re.search(r\"(?<=\\\"duration\\\":)(\\d+)\", soup.text).group(1)\n",
    "        return duration\n",
    "\n",
    "    def get_topic_tags(self, soup):\n",
    "        \"\"\"Returns list of tags (topics) per talk.\"\"\"\n",
    "        match_obj = re.search(r\"\\\"tag\\\":\\\"(.*?)\\\"\", soup.text)\n",
    "        tags = match_obj.group(1).split(',')\n",
    "        return tags\n",
    "\n",
    "    def get_related_talks(self, soup):\n",
    "        \"\"\"Returns dict (keys: id & title) of related talks.\"\"\"\n",
    "        related_tag = re.search(r\"(?<=\\\"related_talks\\\":).*?]\", soup.text).group(0)\n",
    "        related_series = pd.read_json(related_tag)\n",
    "        related_talks = related_series.loc[:, ['id', 'title']].to_dict()\n",
    "        return related_talks\n",
    "\n",
    "    def get_talk_url(self, soup):\n",
    "        \"\"\"Returns url for each talk as a string.\"\"\"\n",
    "        talk_tag = soup.find(attrs={'property': 'og:url'}).attrs['content']\n",
    "        talk_url = talk_tag.split('/transcript')[0]\n",
    "        return talk_url\n",
    "\n",
    "    def get_talk_description(self, soup):\n",
    "        \"\"\"Returns description of the talk.\"\"\"\n",
    "        desc_tag = soup.find(attrs={'property': 'og:description'}).attrs['content']\n",
    "        talk_desc = desc_tag.split(': ', 1)[1]\n",
    "        return talk_desc\n",
    "\n",
    "    def get_transcript(self, soup):\n",
    "        \"\"\"Returns talk's transcript as a single string.\"\"\" \n",
    "        transcript = ''\n",
    "        transcript_strings = []\n",
    "        for div in soup.find_all('div', class_=\"Grid__cell flx-s:1 p-r:4\"):\n",
    "            for p in div.find_all('p'):\n",
    "                # add every string in the transcript to a list\n",
    "                transcript_strings.append(\" \".join(p.text.split()))\n",
    "            else:\n",
    "                # after all strings have been added, create a single transcript string\n",
    "                transcript = \" \".join(transcript_strings)\n",
    "        return transcript\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## TEDscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class TEDscraper(TalkFeatures):\n",
    "    \"\"\"Gets urls and scrapes TED talk data in specified language\n",
    "    Attributes:\n",
    "        lang_code (str): Language code. Defaults to 'en'.\n",
    "        language (str): Language name derived from lang_code.\n",
    "        urls (list): URLs to be scraped. Defaults to 'all'.\n",
    "        exclude (bool): Exclude transcript. Defaults to False.\n",
    "        ted_dict (dict): Dict to store ted talk features after scraping.\n",
    "        dict_id (int): Index of nested dict in 'ted_dict'.\n",
    "        failed_counter: Counts urls that failed to get scraped.\n",
    "    \"\"\"\n",
    "    def __init__(self, lang_code='en', urls='all', exclude_transcript=False):\n",
    "        self.lang_code = lang_code\n",
    "        self.language = self.convert_lang_code()\n",
    "        self.urls = urls\n",
    "        self.exclude = exclude_transcript\n",
    "        self.ted_dict = {}\n",
    "        self.dict_id = 0\n",
    "        self.failed_counter = 0\n",
    "\n",
    "    def get_languages(self):\n",
    "        \"\"\"Returns DataFrame of all language codes supported by TED.\"\"\"\n",
    "        lang_url = 'https://www.ted.com/participate/translate/our-languages'\n",
    "        soup = self.make_soup(lang_url)\n",
    "        lang_list = []\n",
    "        lang_tags = soup.find_all('div', class_='h9')\n",
    "        for tag in lang_tags:\n",
    "            if tag.a == None:\n",
    "                continue\n",
    "            else:\n",
    "                lang_code = re.search(r'(?<=\\=)[\\w-]+', tag.a['href']).group(0)\n",
    "                lang_name = tag.text\n",
    "                lang_list.append([lang_code] + [lang_name])\n",
    "        lang_df = pd.DataFrame(data=lang_list, columns=['code', 'language'])\n",
    "        return lang_df\n",
    "    \n",
    "    def convert_lang_code(self):\n",
    "        \"\"\"Reads languages.csv and returns language.\n",
    "        Parameters:\n",
    "            lang_code (str): Language code\n",
    "        \"\"\"\n",
    "        df = pd.read_csv('../data/languages.csv')\n",
    "        lang_series = df.loc[(df['code'] == self.lang_code), 'language']\n",
    "        language = lang_series.values[0]\n",
    "        return language\n",
    "\n",
    "    def get_max_page(self):\n",
    "        \"\"\"Returns max pagination number from www.ted.com/talks.\"\"\"\n",
    "        page_num = [1]\n",
    "        # make soup from ted.com/talks with specified language\n",
    "        soup = self.make_soup('https://www.ted.com/talks?language='\n",
    "                              + self.lang + '&page=1&sort=newest')\n",
    "        # iterate through each pagination element and get the max\n",
    "        page_elem = soup.find_all('a', class_='pagination__item pagination__link')\n",
    "        for element in page_elem:\n",
    "            page_num.append(int(element.text))\n",
    "        return max(page_num)\n",
    "    \n",
    "    def get_all_url_paths(self):\n",
    "        \"\"\"Returns list of all the talk url paths available in www.ted.com/talks\"\"\"\n",
    "        url_path_list = []\n",
    "        # construct url with lang code specified by the user\n",
    "        talks_url = ('https://www.ted.com/talks?language='\n",
    "                    + self.lang + '&page=')\n",
    "        # set range from 1 to the max page in the pagination element\n",
    "        page_range = range(self.get_max_page())\n",
    "        # iterate through each page and get the url for each talk\n",
    "        for i in page_range:\n",
    "            # try a second attempt if first attempt fails\n",
    "            for attempt in range(2):\n",
    "                try:\n",
    "                    talks_page_url = talks_url + str(i) + '&sort=newest'\n",
    "                    soup = self.make_soup(talks_page_url)\n",
    "                    # delay between searches\n",
    "                    self.sleep_short()\n",
    "                    for div in soup.find_all('div', attrs={'class': 'media__image'}):\n",
    "                        for a in div.find_all('a'):\n",
    "                            url_path_list.append(a.get('href'))\n",
    "                except:\n",
    "                    # delay before continuing to second attempt\n",
    "                    self.sleep_long()\n",
    "                # break from attempts loop if no exceptions are raised\n",
    "                else:\n",
    "                    break\n",
    "        return url_path_list\n",
    "\n",
    "    def get_all_urls(self):\n",
    "        \"\"\"Returns list of complete urls for each talk's transcript page.\"\"\"\n",
    "        url_list = []\n",
    "        for url in self.get_all_url_paths():\n",
    "            url_list.append(('https://www.ted.com'\n",
    "                             + url.replace(\n",
    "                                 # to replace\n",
    "                                 '?language=' + self.lang,\n",
    "                                 # replace with\n",
    "                                 '/transcript' + '?language=' + self.lang)\n",
    "                            ))\n",
    "        return url_list\n",
    "    \n",
    "    def clean_urls(self, urls):\n",
    "        \"\"\"Returns list of clean urls from urls the user inputs.\"\"\"\n",
    "        clean_urls = []\n",
    "        for idx, url in enumerate(urls):\n",
    "            if url.startswith('https://www.ted.com/talks'):\n",
    "                parts = url.split('/')\n",
    "                joined = '/'.join(parts[:5])\n",
    "                clean = joined.split('?')\n",
    "                lang = clean[0] + '/transcript?language=' + self.lang\n",
    "                clean_urls.append(lang)\n",
    "            else:\n",
    "                print(f'bad url @ {idx} >> {url}')\n",
    "                continue\n",
    "        return clean_urls\n",
    "    \n",
    "    def scrape_all_features(self, soup):\n",
    "        \"\"\"Scrapes all features to a nested dict.\"\"\"\n",
    "        # create nested dict\n",
    "        self.ted_dict[self.dict_id] = {}\n",
    "        nested_dict = self.ted_dict[self.dict_id]\n",
    "        # add the features to the nested dict\n",
    "        nested_dict['talk_id'] = self.get_talk_id(soup)\n",
    "        nested_dict['title'] = self.get_title(soup)\n",
    "        nested_dict['speakers'] = self.get_speakers(soup)\n",
    "        nested_dict['occupations'] = self.get_occupations(soup)\n",
    "        nested_dict['about_speakers'] = self.get_about_speakers(soup)\n",
    "        nested_dict['views'] = self.get_views(soup)\n",
    "        nested_dict['recorded_date'] = self.get_recorded_date(soup)\n",
    "        nested_dict['published_date'] = self.get_published_date(soup)\n",
    "        nested_dict['event'] = self.get_event(soup)\n",
    "        nested_dict['native_lang'] = self.get_native_lang(soup)\n",
    "        nested_dict['available_lang'] = self.get_available_lang(soup)\n",
    "        nested_dict['comments'] = self.get_comments_count(soup)\n",
    "        nested_dict['duration'] = self.get_duration(soup)\n",
    "        nested_dict['duration_sec'] = self.get_duration_sec(soup)\n",
    "        nested_dict['topic_tags'] = self.get_topic_tags(soup)\n",
    "        nested_dict['related_talks'] = self.get_related_talks(soup)\n",
    "        nested_dict['talk_url'] = self.get_talk_url(soup)\n",
    "        nested_dict['talk_description'] = self.get_talk_description(soup)\n",
    "        # add transcript if param is set to False (default)\n",
    "        if not self.exclude:\n",
    "            nested_dict['transcript'] = self.get_transcript(soup)\n",
    "        return nested_dict\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"Returns nested dictionary of features from each talk's transcript page.\"\"\"\n",
    "        print(\"Getting all urls...\")\n",
    "        # define url attribute\n",
    "        if self.urls == 'all':\n",
    "            urls = self.get_all_urls()\n",
    "        else:\n",
    "            if isinstance(self.urls, list):\n",
    "                urls = self.clean_urls(self.urls)\n",
    "            else:\n",
    "                print(\"'urls' param needs to be a list\")\n",
    "        print(f\"Scraping {len(urls)} TED talks in '{self.language}'...\")\n",
    "#         print(f\"Estimated scrape time is {(1.5*len(urls)/60)} minutes\\n\")\n",
    "        # iterate through each ted talk transcript url\n",
    "        for url in urls:\n",
    "            # make soup\n",
    "            soup = self.make_soup(url)\n",
    "            # taste soup\n",
    "            taster = soup.title.text\n",
    "            bad_soup = re.search(r'404: Not Found', taster)\n",
    "            if bad_soup:\n",
    "                print(f\"\\nBad soup! TED might not have this talk available in \"\n",
    "                      f\"'{self.lang}'. Check the url\\n{url}\\n\")\n",
    "                self.failed_counter += 1\n",
    "                continue\n",
    "            # delay between searches\n",
    "            self.sleep_short()\n",
    "            # try up to three attempts to scrape data\n",
    "            for attempt in range(1, 3+1):\n",
    "                try:\n",
    "                    # create nested dict\n",
    "                    self.ted_dict[self.dict_id] = {}\n",
    "                    # scrape features and add to a nested dict\n",
    "                    self.scrape_all_features(soup)\n",
    "                    # indicate successful scrape\n",
    "                    print(self.dict_id, url)\n",
    "                    # add 1 to create a new nested dict\n",
    "                    self.dict_id += 1\n",
    "                except Exception as e:\n",
    "                    # if the last attempt fails, update the failed counter\n",
    "                    # and print the exception & talk url\n",
    "                    if attempt == 3:\n",
    "                        self.failed_counter += 1\n",
    "                        print(f'position: {self.dict_id}, exception: {e}, url: {url}\\n')\n",
    "                        continue\n",
    "                    # delay before another attempt\n",
    "                    self.sleep_long()\n",
    "                # break if no exceptions are raised\n",
    "                else:\n",
    "                    break\n",
    "        print(f\"\"\"\\nTed.com scraping results:\n",
    "            \\n\\t• Successful: {self.dict_id}\n",
    "            \\n\\t• Failed: {self.failed_counter}\\n\"\"\")\n",
    "        return self.ted_dict\n",
    "    \n",
    "    def to_dataframe(self, ted_dict):\n",
    "        \"\"\"Creates DataFrame object from dict.\"\"\"\n",
    "        df = pd.DataFrame.from_dict(ted_dict, orient='index')\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Output types\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#serialization-io-conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "scraper = TEDscraper(lang_code='arq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Algerian Arabic'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/languages.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>af</td>\n",
       "      <td>Afrikaans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sq</td>\n",
       "      <td>Albanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arq</td>\n",
       "      <td>Algerian Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>am</td>\n",
       "      <td>Amharic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ar</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>uk</td>\n",
       "      <td>Ukrainian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>ur</td>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>ug</td>\n",
       "      <td>Uyghur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>uz</td>\n",
       "      <td>Uzbek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>vi</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    code         language\n",
       "0     af        Afrikaans\n",
       "1     sq         Albanian\n",
       "2    arq  Algerian Arabic\n",
       "3     am          Amharic\n",
       "4     ar           Arabic\n",
       "..   ...              ...\n",
       "110   uk        Ukrainian\n",
       "111   ur             Urdu\n",
       "112   ug           Uyghur\n",
       "113   uz            Uzbek\n",
       "114   vi       Vietnamese\n",
       "\n",
       "[115 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/languages.csv')\n",
    "df.loc[df['code']==self.lang, 'language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Troubleshooting\n",
    "# exception: Unmatched ''\"' when when decoding 'string', url: https://www.ted.com/talks/michael_anti_behind_the_great_firewall_of_china/transcript?language=bo\n",
    "unmatched_urls = [\n",
    "                  'https://www.ted.com/talks/michael_anti_behind_the_great_firewall_of_china/transcript?language=bo',\n",
    "#                   'https://www.ted.com/talks/isaac_mizrahi_how_the_button_changed_fashion/transcript?language=hi',\n",
    "#                   'https://www.ted.com/talks/herman_narula_the_transformative_power_of_video_games/transcript?language=hi'\n",
    "                 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "unmatched_scraper = TEDscraper(lang='bo', urls=unmatched_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "unmatched_scraper.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# all features\n",
    "unmatched_scraper.get_talk_id(soup)\n",
    "unmatched_scraper.get_title(soup)\n",
    "unmatched_scraper.get_speakers(soup)\n",
    "unmatched_scraper.get_occupations(soup)\n",
    "unmatched_scraper.get_about_speakers(soup)\n",
    "unmatched_scraper.get_views(soup)\n",
    "unmatched_scraper.get_recorded_date(soup)\n",
    "unmatched_scraper.get_published_date(soup)\n",
    "unmatched_scraper.get_event(soup)\n",
    "unmatched_scraper.get_native_lang(soup)\n",
    "unmatched_scraper.get_available_lang(soup)\n",
    "unmatched_scraper.get_comments_count(soup)\n",
    "unmatched_scraper.get_duration(soup)\n",
    "unmatched_scraper.get_duration_sec(soup)\n",
    "unmatched_scraper.get_topic_tags(soup)\n",
    "unmatched_scraper.get_related_talks(soup)\n",
    "unmatched_scraper.get_talk_url(soup)\n",
    "unmatched_scraper.get_talk_description(soup)\n",
    "unmatched_scraper.get_transcript(soup)\n",
    "\n",
    "unmatched_scraper.get_title(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    \"\"\"Returns soup object from a URL.\"\"\"\n",
    "    # generate random user-agent\n",
    "    user_agent = {'User-agent': UserAgent().random}\n",
    "    # request page and make soup\n",
    "    page = requests.get(url, headers=user_agent)\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://www.ted.com/talks/sarah_kaminsky_my_father_the_forger/',\n",
    "    'https://www.ted.com/talks/jorge_drexler_poetry_music_and_identity/transcript',\n",
    "    'https://www.ted.com/talks/sir_ken_robinson_do_schools_kill_creativity/',\n",
    "    'https://www.ted.com/talks/paul_mceuen_and_marc_miskin_tiny_robots_with_giant_potential/transcript',\n",
    "    'https://www.ted.com/talks/antara_raychaudhuri_and_iseult_gillespie_the_legend_of_annapurna_hindu_goddess_of_nourishment/',\n",
    "    'https://www.ted.com/talks/diana_reiss_peter_gabriel_neil_gershenfeld_and_vint_cerf_the_interspecies_internet_an_idea_in_progress/',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "url = ['https://www.ted.com/talks/diana_reiss_peter_gabriel_neil_gershenfeld_and_vint_cerf_the_interspecies_internet_an_idea_in_progress/']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "get_speakers_2(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "test = TEDscraper(urls=url)\n",
    "test.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-24T01:14:56.451074Z",
     "start_time": "2020-02-24T01:14:56.245272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(ted_dict, orient='index')\n",
    "\n",
    "# Pickle DataFrame\n",
    "df.to_pickle('data/first_df.pkl')\n",
    "\n",
    "print(f'Shape: {df.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
