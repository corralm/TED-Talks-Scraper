{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Feature \n",
    "cache files if data exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# TEDscraper Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-02-24T06:56:51.108234Z",
     "start_time": "2020-02-24T06:56:47.569217Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "## Soup Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-04-24T07:19:30.434915Z",
     "start_time": "2020-04-24T07:19:30.420388Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SoupMaker:\n",
    "    \"\"\"Make soup objects and put your machine to sleep.\"\"\"\n",
    "    \n",
    "\n",
    "    def sleep_short(self):\n",
    "        \"\"\"Suspends execution time between 0 - .2 seconds.\"\"\"\n",
    "        return time.sleep(random.uniform(0, .2))\n",
    "\n",
    "    def sleep_long(self):\n",
    "        \"\"\"Suspends execution time between .5 - 2 seconds.\"\"\"\n",
    "        return time.sleep(random.uniform(.5, 2))\n",
    "\n",
    "    def make_soup(self, url):\n",
    "        \"\"\"Returns soup object from a URL.\"\"\"\n",
    "        # generate random user-agent\n",
    "        user_agent = {'User-agent': UserAgent().random}\n",
    "        # request page and make soup\n",
    "        page = requests.get(url, headers=user_agent)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        return soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## CreateCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class CreateCSV(SoupMaker):\n",
    "    \"\"\"Create CSVs of TED topics and languages.\"\"\"\n",
    "\n",
    "\n",
    "    def create_topics_csv(self):\n",
    "        \"\"\"Creates CSV of all topics available from TED.\"\"\"\n",
    "        soup = self.make_soup('https://www.ted.com/topics')\n",
    "        topic_list = []\n",
    "        topic_tag = soup.find_all(class_='d:b', style='line-height:3;')\n",
    "        for tag in topic_tag:\n",
    "            topic = re.sub(r'\\s+', '', tag.text)\n",
    "            topic_list.append(topic)\n",
    "        topics_series = pd.Series(topic_list, name='Topic')\n",
    "        topics_series.to_csv('../data/topics.csv', index=False)\n",
    "\n",
    "    def create_languages_csv(self):\n",
    "        \"\"\"Creates CSV of all language codes supported by TED.\"\"\"\n",
    "        lang_url = 'https://www.ted.com/participate/translate/our-languages'\n",
    "        soup = self.make_soup(lang_url)\n",
    "        lang_list = []\n",
    "        lang_tags = soup.find_all('div', class_='h9')\n",
    "        for tag in lang_tags:\n",
    "            if tag.a == None:\n",
    "                continue\n",
    "            else:\n",
    "                lang_code = re.search(r'(?<=\\=)[\\w-]+', tag.a['href']).group(0)\n",
    "                lang_name = tag.text\n",
    "                lang_list.append([lang_code] + [lang_name])\n",
    "        lang_df = pd.DataFrame(data=lang_list, columns=['lang_code', 'language'])\n",
    "        lang_df.to_csv('../data/languages.csv', index=False)\n",
    "        \n",
    "    def create_dataset(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "## Talk Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-04-24T07:19:35.650462Z",
     "start_time": "2020-04-24T07:19:35.604835Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TalkFeatures(SoupMaker):\n",
    "    \"\"\"Class to get TED talk features.\"\"\"\n",
    "\n",
    "\n",
    "    def get_talk_id(self, soup):\n",
    "        \"\"\"Returns the talk_id provided by TED.\"\"\"\n",
    "        talk_id = re.search(r\"(?<=\\\"current_talk\\\":)\\\"(\\d+)\\\"\", soup.text).group(1)\n",
    "        return talk_id\n",
    "\n",
    "    def get_title(self, soup):\n",
    "        \"\"\"Returns the title of the talk.\"\"\"\n",
    "        title_tag = soup.find(attrs={'name': 'title'}).attrs['content']\n",
    "        tag_list = title_tag.split(':')\n",
    "        title = \":\".join(tag_list[1:]).lstrip()\n",
    "        return title\n",
    "\n",
    "    def get_speakers(self, soup):\n",
    "        \"\"\"Returns dict of all speakers per talk.\"\"\"\n",
    "        speaker_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?\\\"}]\", soup.text)[0]\n",
    "        # convert to DataFrame\n",
    "        speakers_df = pd.read_json(speaker_tag)\n",
    "        full_name_raw = (speakers_df.loc[:, 'firstname'] + ' '\n",
    "                     + speakers_df.loc[:, 'middleinitial'] + ' '\n",
    "                     + speakers_df.loc[:, 'lastname'])\n",
    "        full_name_clean = full_name_raw.str.replace('\\s+', ' ')\n",
    "        # transform series to a dict\n",
    "        speakers = full_name_clean.to_dict()\n",
    "        return speakers\n",
    "\n",
    "    def get_occupations(self, soup):\n",
    "        \"\"\"Returns list of the occupation(s) of the speaker(s) per talk.\"\"\"\n",
    "        occupations_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?\\\"}]\", soup.text)[0]\n",
    "        # convert json to DataFrame\n",
    "        occupations_series = pd.read_json(occupations_tag)['description']\n",
    "        if occupations_series.all():\n",
    "            # clean and create dict\n",
    "            occupations = occupations_series.str.lower().str.split(', ')\n",
    "            occupations = occupations.to_dict()\n",
    "        else:\n",
    "            occupations = None\n",
    "        return occupations\n",
    "\n",
    "    def get_about_speakers(self, soup):\n",
    "        \"\"\"Returns dict with each 'About the Speaker' blurb per talk.\"\"\"\n",
    "        speaker_tag = re.findall(r\"(?<=\\\"speakers\\\":).*?\\\"}]\", soup.text)[0]\n",
    "        # convert to DataFrame\n",
    "        about_series = pd.read_json(speaker_tag)['whotheyare']\n",
    "        if about_series.all():\n",
    "            # transform series to a dict\n",
    "            about_speakers = about_series.to_dict()\n",
    "        else:\n",
    "            about_speakers = None\n",
    "        return about_speakers\n",
    "\n",
    "    def get_views(self, soup):\n",
    "        \"\"\"Returns viewed count per talk.\"\"\"\n",
    "        view_count = re.search(r\"(?<=\\\"viewed_count\\\":)\\d+\", soup.text).group(0)\n",
    "        return view_count\n",
    "\n",
    "    def get_recorded_date(self, soup):\n",
    "        \"\"\"Returns date a talk was recorded.\"\"\"\n",
    "        recorded_at = re.search(r\"(?<=\\\"recorded_at\\\":)\\\"(.*?)T\", soup.text).group(1)\n",
    "        return recorded_at\n",
    "\n",
    "    def get_published_date(self, soup):\n",
    "        \"\"\"Returns date a talk was published in TED.com.\"\"\"\n",
    "        published_at = soup.find(attrs={'itemprop': 'uploadDate'}).attrs['content']\n",
    "        return published_at\n",
    "\n",
    "    def get_event(self, soup):\n",
    "        \"\"\"Returns name of the event in which the talk was given.\"\"\"\n",
    "        event = re.search(r\"(?<=\\\"event\\\":)\\\"(.*?)\\\"\", soup.text).group(1)\n",
    "        return event\n",
    "    \n",
    "    def get_native_lang(self, soup):\n",
    "        \"\"\"Returns native language code for each talk as a string.\"\"\"\n",
    "        native_lang = re.search(r'(?<=nativeLanguage\\\":\\\")[\\w-]+', soup.text).group(0)\n",
    "        return native_lang\n",
    "    \n",
    "    def get_available_lang(self, soup):\n",
    "        \"\"\"Returns list of all available languages (lang codes) for a talk.\"\"\"\n",
    "        languages = re.findall(r'(?<=languageCode\\\":\\\")[\\w-]+', soup.text)\n",
    "        clean_lang = sorted(list(set(languages)))\n",
    "        return clean_lang\n",
    "\n",
    "    def get_comments_count(self, soup):\n",
    "        \"\"\"Return the count of comments per talk.\"\"\"\n",
    "        try:\n",
    "            comments_count = re.search(r\"(?<=\\\"count\\\":)(\\d+)\", soup.text).group(1)\n",
    "        except AttributeError:\n",
    "            comments_count = None\n",
    "        return comments_count\n",
    "\n",
    "    def get_duration(self, soup):\n",
    "        \"\"\"Returns duration of a talk (format ex: 12M43S)\"\"\"\n",
    "        duration_tag = soup.find(attrs={'itemprop': 'duration'}).attrs['content']\n",
    "        duration = duration_tag.split('PT')[1]\n",
    "        return duration\n",
    "\n",
    "    def get_duration_sec(self, soup):\n",
    "        \"\"\"Returns duration of a talk in seconds.\"\"\"\n",
    "        duration =  re.search(r\"(?<=\\\"duration\\\":)(\\d+)\", soup.text).group(1)\n",
    "        return duration\n",
    "\n",
    "    def get_topic_tags(self, soup):\n",
    "        \"\"\"Returns list of tags (topics) per talk.\"\"\"\n",
    "        match_obj = re.search(r\"\\\"tag\\\":\\\"(.*?)\\\"\", soup.text)\n",
    "        tags = match_obj.group(1).split(',')\n",
    "        return tags\n",
    "\n",
    "    def get_related_talks(self, soup):\n",
    "        \"\"\"Returns dict (keys: id & title) of related talks.\"\"\"\n",
    "        related_tag = re.search(r\"(?<=\\\"related_talks\\\":).*?]\", soup.text).group(0)\n",
    "        related_series = pd.read_json(related_tag)\n",
    "        related_talks = related_series.loc[:, ['id', 'title']].to_dict()\n",
    "        return related_talks\n",
    "\n",
    "    def get_talk_url(self, soup):\n",
    "        \"\"\"Returns url for each talk as a string.\"\"\"\n",
    "        talk_tag = soup.find(attrs={'property': 'og:url'}).attrs['content']\n",
    "        talk_url = talk_tag.split('/transcript')[0]\n",
    "        return talk_url\n",
    "\n",
    "    def get_talk_description(self, soup):\n",
    "        \"\"\"Returns description of the talk.\"\"\"\n",
    "        desc_tag = soup.find(attrs={'property': 'og:description'}).attrs['content']\n",
    "        talk_desc = desc_tag.split(': ', 1)[1]\n",
    "        return talk_desc\n",
    "\n",
    "    def get_transcript(self, soup):\n",
    "        \"\"\"Returns talk's transcript as a single string.\"\"\" \n",
    "        transcript = ''\n",
    "        transcript_strings = []\n",
    "        for div in soup.find_all('div', class_=\"Grid__cell flx-s:1 p-r:4\"):\n",
    "            for p in div.find_all('p'):\n",
    "                # add every string in the transcript to a list\n",
    "                transcript_strings.append(\" \".join(p.text.split()))\n",
    "            else:\n",
    "                # after all strings have been added, create a single transcript string\n",
    "                transcript = \" \".join(transcript_strings)\n",
    "        return transcript\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "## URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-04-24T07:19:36.214636Z",
     "start_time": "2020-04-24T07:19:36.181907Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class URLs(SoupMaker):\n",
    "    \"\"\"Get and process urls to scrape.\"\"\"\n",
    "\n",
    "    \n",
    "    def topics_url_param(self):\n",
    "        \"\"\"Returns string of the url query from topics parameter.\"\"\"\n",
    "        topics_param = ''\n",
    "        if self.topics != 'all':\n",
    "            if isinstance(self.topics, list):\n",
    "                for topic in self.topics:\n",
    "                    topics_param += ('&topics[]=' + topic)\n",
    "            else:\n",
    "                raise ValueError(\"'topics' param needs to be a list\")\n",
    "        return topics_param\n",
    "\n",
    "    def get_max_page(self):\n",
    "        \"\"\"Returns max pagination number from www.ted.com/talks.\"\"\"\n",
    "        page_num = [1]\n",
    "        # make soup from ted.com/talks with specified language\n",
    "        soup = self.make_soup(self.base_url + '&page=1&sort=newest')\n",
    "        # iterate through each pagination element and get the max\n",
    "        page_elem = soup.find_all('a', class_='pagination__item pagination__link')\n",
    "        for element in page_elem:\n",
    "            page_num.append(int(element.text))\n",
    "        return max(page_num)\n",
    "    \n",
    "    def get_all_url_paths(self):\n",
    "        \"\"\"Returns list of all the talk url paths available in www.ted.com/talks\"\"\"\n",
    "        url_path_list = []\n",
    "        # construct url with lang code specified by the user\n",
    "        talks_url = (self.base_url + '&page=')\n",
    "        # set range from 1 to the max page in the pagination element\n",
    "        page_range = range(1, self.get_max_page()+1)\n",
    "        # iterate through each page and get the url for each talk\n",
    "        for i in page_range:\n",
    "            # try a second attempt if first attempt fails\n",
    "            for attempt in range(2):\n",
    "                try:\n",
    "                    talks_page_url = talks_url + str(i) + '&sort=newest'\n",
    "                    soup = self.make_soup(talks_page_url)\n",
    "                    # delay between searches\n",
    "                    self.sleep_short()\n",
    "                    for div in soup.find_all('div', attrs={'class': 'media__image'}):\n",
    "                        for a in div.find_all('a'):\n",
    "                            url_path_list.append(a.get('href'))\n",
    "                except:\n",
    "                    # delay before continuing to second attempt\n",
    "                    self.sleep_long()\n",
    "                # break from attempts loop if no exceptions are raised\n",
    "                else:\n",
    "                    break\n",
    "        return url_path_list\n",
    "\n",
    "    def get_all_urls(self):\n",
    "        \"\"\"Returns list of complete urls for each talk's transcript page.\"\"\"\n",
    "        # '/talks/jen_gunter_why_can_t_we_talk_about_periods?language=fa'\n",
    "        url_list = []\n",
    "        for url in self.get_all_url_paths():\n",
    "            url_list.append(('https://www.ted.com'\n",
    "                             + url.replace(\n",
    "                                 # to replace\n",
    "                                 '?language=' + self.lang_code,\n",
    "                                 # replace with\n",
    "                                 '/transcript' + '?language=' + self.lang_code)\n",
    "                            ))\n",
    "        return url_list\n",
    "    \n",
    "    def clean_urls(self, urls):\n",
    "        \"\"\"Returns list of clean urls from urls the user inputs.\"\"\"\n",
    "        clean_urls = []\n",
    "        for idx, url in enumerate(urls):\n",
    "            if url.startswith('https://www.ted.com/talks'):\n",
    "                parts = url.split('/')\n",
    "                joined = '/'.join(parts[:5])\n",
    "                clean = joined.split('?')\n",
    "                lang = clean[0] + '/transcript?language=' + self.lang_code\n",
    "                topic = lang + self.topics_url_param()\n",
    "                clean_urls.append(lang)\n",
    "            else:\n",
    "                print(f'bad url @ {idx} >> {url}')\n",
    "                continue\n",
    "        return clean_urls\n",
    "    \n",
    "    def url_issues(self):\n",
    "        \"\"\"Returns DataFrame of urls with known issues.\"\"\"\n",
    "        issues_df = pd.read_csv('../data/urls_issues.csv')\n",
    "        return issues_df\n",
    "    \n",
    "    def remove_urls_with_issues(self):\n",
    "        \"\"\"Remove urls with known issues to prevent unnecessary scraping.\"\"\"\n",
    "        urls = self.url_attribute()\n",
    "        final_urls = []\n",
    "        removed_urls = []\n",
    "        removed_counter = 0\n",
    "        issues_df = pd.read_csv('../data/urls_issues.csv')\n",
    "        for url in urls:\n",
    "            try:\n",
    "                base_url = url.replace('transcript?language=' + self.lang_code, '')\n",
    "                # is base url in the issues df?\n",
    "                url_in_issues = (issues_df['url'] == base_url).any()\n",
    "                # get the lang_codes of the base_url\n",
    "                langs = issues_df.loc[issues_df['url'] == base_url, 'lang_code']\n",
    "                # check if the url in issues_df\n",
    "                if not url_in_issues:\n",
    "                    final_urls.append(url)\n",
    "                # if the url is in issues_df, check if it's for the same lang_code\n",
    "                elif self.lang_code in langs.any():\n",
    "                    removed_urls.append(url)\n",
    "                    removed_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    final_urls.append(url)\n",
    "            except:\n",
    "                removed_urls.append(url)\n",
    "                removed_counter += 1\n",
    "                continue\n",
    "        if removed_urls:\n",
    "            print(f\"Removed the following {removed_counter} urls as they have \"\n",
    "                  \"known issues:\\n\", removed_urls, end='\\n\\n')\n",
    "        return final_urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## TEDscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2020-04-24T07:32:19.545188Z",
     "start_time": "2020-04-24T07:32:19.494574Z"
    }
   },
   "outputs": [],
   "source": [
    "class TEDscraper(TalkFeatures, URLs):\n",
    "    \"\"\"Gets urls and scrapes TED talk data in the specified language.\n",
    "\n",
    "    Attributes:\n",
    "        lang_code (str): Language code. Defaults to 'en'.\n",
    "        language (str): Language name derived from lang_code.\n",
    "        urls (list): URLs of talks. Defaults to 'all'.\n",
    "        topics (list): Talk topics. Defaults to 'all'.\n",
    "        exclude (bool): Exclude transcript. Defaults to False.\n",
    "        ted_dict (dict): Dict to store ted talk features after scraping.\n",
    "        dict_id (int): Index of nested dict in 'ted_dict'.\n",
    "        failed_counter: Counts urls that failed to get scraped.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, lang_code='en', urls='all', topics='all', exclude_transcript=False):\n",
    "        self.lang_code = lang_code\n",
    "        self.language = self.convert_lang_code()\n",
    "        self.urls = urls\n",
    "        self.topics = topics\n",
    "        self.exclude = exclude_transcript\n",
    "        self.ted_dict = {}\n",
    "        self.dict_id = 0\n",
    "        self.failed_counter = 0\n",
    "        self.base_url = ('https://www.ted.com/talks'\n",
    "                         + '?language=' + self.lang_code\n",
    "                         + self.topics_url_param())\n",
    "        \n",
    "    def url_attribute(self):\n",
    "        \"\"\"Define urls attribute based on parameter 'urls'.\"\"\"\n",
    "        # define url attribute\n",
    "        if self.urls == 'all':\n",
    "            urls = self.get_all_urls()\n",
    "        else:\n",
    "            if isinstance(self.urls, list):\n",
    "                urls = self.clean_urls(self.urls)\n",
    "            else:\n",
    "                raise ValueError(\"'urls' param needs to be a list\")\n",
    "        return urls\n",
    "\n",
    "    def convert_lang_code(self):\n",
    "        \"\"\"Reads languages.csv and returns language.\n",
    "        Parameters:\n",
    "            lang_code (str): Language code\n",
    "        \"\"\"\n",
    "        df = pd.read_csv('../data/languages.csv')\n",
    "        lang_series = df.loc[(df['lang_code'] == self.lang_code), 'language']\n",
    "        language = lang_series.values[0]\n",
    "        return language\n",
    "    \n",
    "    def scrape_all_features(self, soup):\n",
    "        \"\"\"Scrapes all features to a nested dict.\"\"\"\n",
    "        # create nested dict\n",
    "        self.ted_dict[self.dict_id] = {}\n",
    "        nested_dict = self.ted_dict[self.dict_id]\n",
    "        # add the features to the nested dict\n",
    "        nested_dict['talk_id'] = self.get_talk_id(soup)\n",
    "        nested_dict['title'] = self.get_title(soup)\n",
    "        nested_dict['speakers'] = self.get_speakers(soup)\n",
    "        nested_dict['occupations'] = self.get_occupations(soup)\n",
    "        nested_dict['about_speakers'] = self.get_about_speakers(soup)\n",
    "        nested_dict['views'] = self.get_views(soup)\n",
    "        nested_dict['recorded_date'] = self.get_recorded_date(soup)\n",
    "        nested_dict['published_date'] = self.get_published_date(soup)\n",
    "        nested_dict['event'] = self.get_event(soup)\n",
    "        nested_dict['native_lang'] = self.get_native_lang(soup)\n",
    "        nested_dict['available_lang'] = self.get_available_lang(soup)\n",
    "        nested_dict['comments'] = self.get_comments_count(soup)\n",
    "        nested_dict['duration'] = self.get_duration(soup)\n",
    "        nested_dict['duration_sec'] = self.get_duration_sec(soup)\n",
    "        nested_dict['topic_tags'] = self.get_topic_tags(soup)\n",
    "        nested_dict['related_talks'] = self.get_related_talks(soup)\n",
    "        nested_dict['talk_url'] = self.get_talk_url(soup)\n",
    "        nested_dict['talk_description'] = self.get_talk_description(soup)\n",
    "        # add transcript if param is set to False (default)\n",
    "        if not self.exclude:\n",
    "            nested_dict['transcript'] = self.get_transcript(soup)\n",
    "        return nested_dict\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"Returns nested dictionary of features from each talk's transcript page.\"\"\"\n",
    "        print(\"Fetching all urls...\")\n",
    "        # define url attribute\n",
    "        urls = self.remove_urls_with_issues()\n",
    "        print(f\"Scraping {len(urls)} TED talks in '{self.language}'...\")\n",
    "        print(f\"Estimated time to complete is {round((.9*len(urls)/60), 1)} minutes\\n\")\n",
    "        # iterate through each ted talk transcript url\n",
    "        for url in urls:\n",
    "            # make soup\n",
    "            soup = self.make_soup(url)\n",
    "            # taste soup\n",
    "            taster = soup.title.text\n",
    "            bad_soup = re.search(r'404: Not Found', taster)\n",
    "            if bad_soup:\n",
    "                print(f\"\\nBad soup! TED might not have this talk available in \"\n",
    "                      f\"'{self.lang_code}'. Check the url\\n{url}\\n\")\n",
    "                self.failed_counter += 1\n",
    "                continue\n",
    "            # delay between searches\n",
    "            self.sleep_short()\n",
    "            # try up to three attempts to scrape data\n",
    "            for attempt in range(1, 3+1):\n",
    "                try:\n",
    "                    # create nested dict\n",
    "                    self.ted_dict[self.dict_id] = {}\n",
    "                    # scrape features and add to a nested dict\n",
    "                    self.scrape_all_features(soup)\n",
    "                    # indicate successful scrape\n",
    "                    print(self.dict_id, url)\n",
    "                    # add 1 to create a new nested dict\n",
    "                    self.dict_id += 1\n",
    "                except Exception as e:\n",
    "                    # if the last attempt fails, update the failed counter\n",
    "                    # and print the exception & talk url\n",
    "                    if attempt == 3:\n",
    "                        self.failed_counter += 1\n",
    "                        print(f'position: {self.dict_id}, exception: {e}, url: {url}\\n')\n",
    "                        continue\n",
    "                    # delay before another attempt\n",
    "                    self.sleep_long()\n",
    "                # break if no exceptions are raised\n",
    "                else:\n",
    "                    break\n",
    "        print(f\"\"\"\\nTed.com scraping results:\n",
    "            \\n\\t• Successful: {self.dict_id}\n",
    "            \\n\\t• Failed: {self.failed_counter}\\n\"\"\")\n",
    "        return self.ted_dict\n",
    "    \n",
    "    def to_dataframe(self, ted_dict):\n",
    "        \"\"\"Creates DataFrame object from dict.\"\"\"\n",
    "        df = pd.DataFrame.from_dict(ted_dict, orient='index')\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "1. Scrape a few talks\n",
    "2. Create a dataset\n",
    "3. Do a 2nd a run and have script skip urls that are in the cache file or dataset\n",
    "4. Scrape non-cached files\n",
    "5. For the talks scraped, add them to the dataset & cache file\n",
    "6. Merge dataframes so the user has everything that was asked for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# scrape topics='Sun' as it only has 6 talks\n",
    "scraper = TEDscraper(topics=['Sun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My wish: Use art to turn the world inside out'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.get_title(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    # should be removed\n",
    "    'https://www.ted.com/talks/zohreh_davoudi_are_we_living_in_a_simulation/',\n",
    "    'https://www.ted.com/talks/marcus_bullock_an_app_that_helps_incarcerated_people_stay_connected_to_their_families/transcript?language=en', # only in 'en'\n",
    "    'https://www.ted.com/talks/anton_garcia_abril_how_prefab_homes_can_transform_affordable_housing/',\n",
    "    \n",
    "    # good ones\n",
    "    'https://www.ted.com/talks/jorge_drexler_poetry_music_and_identity/transcript?language=en',\n",
    "    'https://www.ted.com/talks/sarah_kaminsky_my_father_the_forger/',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "scraper = TEDscraper(urls=urls, lang_code='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "scraper.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# After scraping, add url to a file + lang_code\n",
    "# Data will need to be saved somewhere\n",
    "\n",
    "def add_cache(df, lang_code='fr'):\n",
    "    \"\"\"Add scraped urls to CSV.\"\"\"\n",
    "    cached_df = pd.read_csv('../data/urls_test_cached.csv')\n",
    "    cached_urls = cached_df['url'].array\n",
    "    for url in df['talk_url']:\n",
    "        if url in cached_urls: # and lang_code in cached_langs:\n",
    "            cached_langs = cached_df.loc[cached_df['url'] == url, 'lang_code']\n",
    "            for code in cached_langs:\n",
    "                if lang_code not in code:\n",
    "                    # new lang_code\n",
    "                    cached_langs\n",
    "                    # add code to lang_code list\n",
    "                    cached_langs.update(pd.Series(['test'], \n",
    "                else:\n",
    "                    # get the data from dataset\n",
    "                    pass\n",
    "        else:\n",
    "            cached_df.append(url, list(lang_code))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Solution:\n",
    "When scraping, don't save them as a list, just do a string (en, es, fr, etc.) in which I can then use s.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cached_langs = cached_df.loc[cached_df['url'] == url, 'lang_code']\n",
    "for code in cached_langs:\n",
    "    if 'en' in code:\n",
    "        print('pass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Fix duplicate titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "soup = make_soup('https://www.ted.com/talks/jr_my_wish_use_art_to_turn_the_world_inside_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do schools kill creativity?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_title(soup):\n",
    "    \"\"\"Returns the title of the talk.\"\"\"\n",
    "    title_tag = soup.find(attrs={'name': 'title'}).attrs['content']\n",
    "    tag_list = title_tag.split(':')\n",
    "    title = \":\".join(tag_list[1:]).lstrip()\n",
    "    return title\n",
    "\n",
    "get_title(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "s = \"-\";\n",
    "seq = (\"a\", \"b\", \"c\"); # This is sequence of strings.\n",
    "print s.join( seq )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Support Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    \"\"\"Returns soup object from a URL.\"\"\"\n",
    "    # generate random user-agent\n",
    "    user_agent = {'User-agent': UserAgent().random}\n",
    "    # request page and make soup\n",
    "    page = requests.get(url, headers=user_agent)\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# all features\n",
    "scraper.get_talk_id(soup)\n",
    "scraper.get_title(soup)\n",
    "scraper.get_speakers(soup)\n",
    "scraper.get_occupations(soup)\n",
    "scraper.get_about_speakers(soup)\n",
    "scraper.get_views(soup)\n",
    "scraper.get_recorded_date(soup)\n",
    "scraper.get_published_date(soup)\n",
    "scraper.get_event(soup)\n",
    "scraper.get_native_lang(soup)\n",
    "scraper.get_available_lang(soup)\n",
    "scraper.get_comments_count(soup)\n",
    "scraper.get_duration(soup)\n",
    "scraper.get_duration_sec(soup)\n",
    "scraper.get_topic_tags(soup)\n",
    "scraper.get_related_talks(soup)\n",
    "scraper.get_talk_url(soup)\n",
    "scraper.get_talk_description(soup)\n",
    "scraper.get_transcript(soup)\n",
    "\n",
    "scraper.get_title(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# duplicate titles\n",
    "dupe_titles = [\n",
    "       'https://www.ted.com/talks/mark_kendall_demo_a_needle_free_vaccine_patch_that_s_safer_and_way_cheaper',\n",
    "       'https://www.ted.com/talks/nick_sears_demo_the_orb',\n",
    "       'https://www.ted.com/talks/joy_lin_if_superpowers_were_real_invisibility',\n",
    "       'https://www.ted.com/talks/joy_lin_if_superpowers_were_real_flight',\n",
    "       'https://www.ted.com/talks/joy_lin_if_superpowers_were_real_immortality',\n",
    "       'https://www.ted.com/talks/joy_lin_if_superpowers_were_real_super_strength',\n",
    "       'https://www.ted.com/talks/joy_lin_if_superpowers_were_real_super_speed',\n",
    "       'https://www.ted.com/talks/joy_lin_if_superpowers_were_real_body_mass',\n",
    "       'https://www.ted.com/talks/charmian_gooch_my_wish_to_launch_a_new_era_of_openness_in_business',\n",
    "       'https://www.ted.com/talks/jr_my_wish_use_art_to_turn_the_world_inside_out',\n",
    "       'https://www.ted.com/talks/sylvia_earle_my_wish_protect_our_oceans',\n",
    "       'https://www.ted.com/talks/neil_turok_my_wish_find_the_next_einstein_in_africa',\n",
    "       'https://www.ted.com/talks/karen_armstrong_my_wish_the_charter_for_compassion',\n",
    "       'https://www.ted.com/talks/dave_eggers_my_wish_once_upon_a_school',\n",
    "       'https://www.ted.com/talks/james_nachtwey_my_wish_let_my_photographs_bear_witness',\n",
    "       'https://www.ted.com/talks/bill_clinton_my_wish_rebuilding_rwanda',\n",
    "       'https://www.ted.com/talks/e_o_wilson_my_wish_build_the_encyclopedia_of_life',\n",
    "       'https://www.ted.com/talks/bono_my_wish_three_actions_for_africa',\n",
    "       'https://www.ted.com/talks/robert_fischell_my_wish_three_unusual_medical_inventions',\n",
    "       'https://www.ted.com/talks/edward_burtynsky_my_wish_manufactured_landscapes_and_green_education',\n",
    "       'https://www.ted.com/talks/cameron_sinclair_my_wish_a_call_for_open_source_architecture',\n",
    "       'https://www.ted.com/talks/jehane_noujaim_my_wish_a_global_day_of_film',\n",
    "       'https://www.ted.com/talks/larry_brilliant_my_wish_help_me_stop_pandemics'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Test urls\n",
    "urls = [\n",
    "    'https://www.ted.com/talks/sarah_kaminsky_my_father_the_forger/',\n",
    "    'https://www.ted.com/talks/jorge_drexler_poetry_music_and_identity/transcript',\n",
    "    'https://www.ted.com/talks/sir_ken_robinson_do_schools_kill_creativity/',\n",
    "    'https://www.ted.com/talks/paul_mceuen_and_marc_miskin_tiny_robots_with_giant_potential/transcript',\n",
    "    'https://www.ted.com/talks/antara_raychaudhuri_and_iseult_gillespie_the_legend_of_annapurna_hindu_goddess_of_nourishment/',\n",
    "    'https://www.ted.com/talks/diana_reiss_peter_gabriel_neil_gershenfeld_and_vint_cerf_the_interspecies_internet_an_idea_in_progress/',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "errors = [\n",
    "    'https://www.ted.com/talks/marcus_bullock_an_app_that_helps_incarcerated_people_stay_connected_to_their_families/transcript?language=en',\n",
    "    'https://www.ted.com/talks/marilyn_waring_the_unpaid_work_that_gdp_ignores_and_why_it_really_counts/',\n",
    "    'https://www.ted.com/talks/zohreh_davoudi_are_we_living_in_a_simulation/',\n",
    "    'https://www.ted.com/talks/maisie_williams_why_talent_carries_you_further_than_fame/',\n",
    "    'https://www.ted.com/talks/keith_lowe_why_we_need_to_stop_obsessing_over_world_war_ii/',\n",
    "    'https://www.ted.com/talks/anton_garcia_abril_how_prefab_homes_can_transform_affordable_housing/',\n",
    "]\n",
    "errors_sr = pd.Series(errors, name='errors_urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "errors_sr.to_csv('../data/urls_errors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Known Issues\n",
    "\n",
    "#### Talks that weren't scraped\n",
    "* Talk is in English but has no English transcript\n",
    "    * https://www.ted.com/talks/marcus_bullock_an_app_that_helps_incarcerated_people_stay_connected_to_their_families/transcript?language=en\n",
    "* No 'recorded_date'\n",
    "    * https://www.ted.com/talks/marilyn_waring_the_unpaid_work_that_gdp_ignores_and_why_it_really_counts/\n",
    "* Speakers info is missing\n",
    "    * The name is available elsewhere, but not the occupation\n",
    "    * https://www.ted.com/talks/zohreh_davoudi_are_we_living_in_a_simulation/\n",
    "    * https://www.ted.com/talks/maisie_williams_why_talent_carries_you_further_than_fame/\n",
    "    * https://www.ted.com/talks/keith_lowe_why_we_need_to_stop_obsessing_over_world_war_ii/\n",
    "    * https://www.ted.com/talks/anton_garcia_abril_how_prefab_homes_can_transform_affordable_housing/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
